"""
Qwen LLM bmodel é€‚é…å™¨ - ä½¿ç”¨HTTP APIè®¿é—®è¿œç¨‹æ¨¡å‹
ä¸º LightRAG æä¾›æœ¬åœ° Qwen bmodel æ¨ç†æ¥å£
"""
import sys
import os
import asyncio
import signal
import requests
import json
import time
from typing import List, Optional, AsyncIterator

# å¯¼å…¥ç®€åŒ–çš„å…±äº«ç®¡ç†å™¨ç”¨äºçŠ¶æ€æ£€æŸ¥
sys.path.append('/data/whisper-TPU_py/bmwhisper')
from shared_qwen_manager import simple_qwen

# ============ æ¨¡å‹è·¯å¾„é…ç½® ============
MODEL_CONFIG = {
    "llm_model_path": "/data/qwen4btune_w4bf16_seq8192_bm1684x_1dev_20250721_195513.bmodel",
    "config_path": "/data/LLM-TPU/models/Qwen3/python_demo/config",
    "device_id": "0",
    "temperature": 0.5,
    "top_p": 1.0,
    "repeat_penalty": 1.0,
    "repeat_last_n": 32,
    "max_new_tokens": 512,
    "generation_mode": "greedy"
}
# =====================================

# APIé…ç½®
API_BASE_URL = "http://localhost:8899"

class QwenLLMAdapter:
    """Qwen LLM bmodel é€‚é…å™¨ç±» - é€šè¿‡HTTP APIè®¿é—®æ¨¡å‹"""
    
    def __init__(self, **kwargs):
        self.config = {
            "model_path": kwargs.get("model_path", MODEL_CONFIG["llm_model_path"]),
            "config_path": kwargs.get("config_path", MODEL_CONFIG["config_path"]),
            "device_id": kwargs.get("device_id", MODEL_CONFIG["device_id"]),
            "temperature": kwargs.get("temperature", MODEL_CONFIG["temperature"]),
            "top_p": kwargs.get("top_p", MODEL_CONFIG["top_p"]),
            "repeat_penalty": kwargs.get("repeat_penalty", MODEL_CONFIG["repeat_penalty"]),
            "repeat_last_n": kwargs.get("repeat_last_n", MODEL_CONFIG["repeat_last_n"]),
            "max_new_tokens": kwargs.get("max_new_tokens", MODEL_CONFIG["max_new_tokens"]),
            "generation_mode": kwargs.get("generation_mode", MODEL_CONFIG["generation_mode"]),
            "prompt_mode": "prompted",
            "enable_history": False
        }
        
        self._check_model_availability()
    
    def _check_api_server(self, timeout=10):
        """æ£€æŸ¥APIæœåŠ¡å™¨æ˜¯å¦å¯ç”¨"""
        try:
            response = requests.get(f"{API_BASE_URL}/status", timeout=timeout)
            if response.status_code == 200:
                status = response.json()
                return True, status
            else:
                return False, f"HTTP {response.status_code}"
        except requests.exceptions.Timeout:
            return False, "è¿æ¥è¶…æ—¶"
        except requests.exceptions.ConnectionError:
            return False, "è¿æ¥æ‹’ç»"
        except Exception as e:
            return False, str(e)
    
    def _check_model_availability(self):
        """æ£€æŸ¥æ¨¡å‹å¯ç”¨æ€§"""
        # é¦–å…ˆæ£€æŸ¥ç®€åŒ–çš„å…±äº«ç®¡ç†å™¨çŠ¶æ€
        status = simple_qwen.get_status()
        print(f"å…±äº«ç®¡ç†å™¨çŠ¶æ€: {status}")
        
        if status["external_model"]:
            print(f"âœ… æ£€æµ‹åˆ°å¤–éƒ¨è¿›ç¨‹(PID: {status['external_pid']})å·²åŠ è½½æ¨¡å‹")
            
            # æ£€æŸ¥APIæœåŠ¡å™¨æ˜¯å¦å¯ç”¨
            api_available, api_status = self._check_api_server(timeout=10)
            if api_available:
                print("âœ… APIæœåŠ¡å™¨å¯ç”¨ï¼Œå¯ä»¥è¿›è¡Œæ¨ç†")
            else:
                print(f"âš ï¸ APIæœåŠ¡å™¨ä¸å¯ç”¨: {api_status}")
                print("è¯·ç¡®ä¿ sample_audio.py æ­£åœ¨è¿è¡Œ")
       
        else:
            print("âš ï¸ æœªæ£€æµ‹åˆ°å·²åŠ è½½çš„æ¨¡å‹")
            print("è¯·å…ˆè¿è¡Œ sample_audio.py åŠ è½½æ¨¡å‹")
    
    def _call_api(self, prompt: str, max_new_tokens: int = 512, retries=3) -> str:
        """è°ƒç”¨APIè¿›è¡Œæ¨ç†ï¼Œå¸¦é‡è¯•æœºåˆ¶"""
        
        for attempt in range(retries):
            try:
                print(f"ğŸ“¡ å‘é€APIè¯·æ±‚ (å°è¯• {attempt + 1}/{retries})")
                
                data = {
                    "prompt": prompt,
                    "max_new_tokens": max_new_tokens
                }
                
                # å¢åŠ è¶…æ—¶æ—¶é—´
                response = requests.post(
                    f"{API_BASE_URL}/generate",
                    json=data,
                    timeout=180  # 3åˆ†é’Ÿè¶…æ—¶
                )
                
                if response.status_code == 200:
                    result = response.json()
                    if result.get("status") == "success":
                        print("âœ… APIè¯·æ±‚æˆåŠŸ")
                        return result.get("result", "")
                    else:
                        error_msg = result.get('error', 'Unknown error')
                        print(f"âŒ APIè¿”å›é”™è¯¯: {error_msg}")
                        if attempt < retries - 1:
                            print(f"ğŸ”„ {5}ç§’åé‡è¯•...")
                            time.sleep(5)
                            continue
                        return f"API Error: {error_msg}"
                else:
                    error_msg = f"HTTP {response.status_code}"
                    print(f"âŒ HTTPé”™è¯¯: {error_msg}")
                    if attempt < retries - 1:
                        print(f"ğŸ”„ {3}ç§’åé‡è¯•...")
                        time.sleep(3)
                        continue
                    return f"HTTP Error: {error_msg}"
                    
            except requests.exceptions.ConnectionError:
                error_msg = "æ— æ³•è¿æ¥åˆ°APIæœåŠ¡å™¨"
                print(f"âŒ è¿æ¥é”™è¯¯: {error_msg}")
                if attempt < retries - 1:
                    print(f"ğŸ”„ {3}ç§’åé‡è¯•...")
                    time.sleep(3)
                    continue
                return f"Error: {error_msg}ã€‚è¯·ç¡®ä¿ sample_audio.py æ­£åœ¨è¿è¡Œã€‚"
            except requests.exceptions.Timeout:
                error_msg = "APIè¯·æ±‚è¶…æ—¶"
                print(f"âŒ è¶…æ—¶é”™è¯¯: {error_msg}")
                if attempt < retries - 1:
                    print(f"ğŸ”„ {5}ç§’åé‡è¯•...")
                    time.sleep(5)
                    continue
                return f"Error: {error_msg}"
            except Exception as e:
                error_msg = str(e)
                print(f"âŒ å…¶ä»–é”™è¯¯: {error_msg}")
                if attempt < retries - 1:
                    print(f"ğŸ”„ {3}ç§’åé‡è¯•...")
                    time.sleep(3)
                    continue
                return f"Error: {error_msg}"
        
        return "Error: æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†"
    
    def generate(self, prompt: str, system_prompt: str = None, 
                history_messages: list = None, stream: bool = False, **kwargs) -> str:
        """ç”Ÿæˆå›å¤ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰"""
        try:
            # æ„å»ºå®Œæ•´çš„æç¤º
            full_prompt = prompt
            if system_prompt:
                full_prompt = f"System: {system_prompt}\nUser: {prompt}"
            
            
            status = simple_qwen.get_status()
            
            # ä½¿ç”¨APIè°ƒç”¨
            print("ğŸ”„ ä½¿ç”¨APIæ¨ç†...")
            return self._call_api(
                full_prompt,
                kwargs.get('max_new_tokens', self.config['max_new_tokens'])
            )
            
        except Exception as e:
            return f"Error: {str(e)}"
    
    async def agenerate(self, prompt: str, system_prompt: str = None, 
                   history_messages: list = None, stream: bool = False, **kwargs) -> str:
        if stream:
            # è¿™ä¸ªåˆ†æ”¯å®é™…ä¸Šä¸ä¼šè¢« lightrag_bmodel2.py è°ƒç”¨ï¼Œä½†ä¿æŒé€»è¾‘å®Œæ•´
            raise NotImplementedError("Use agenerate_stream directly for streaming.")

        response_parts = []
        async for chunk in self.agenerate_stream(prompt, system_prompt, history_messages, **kwargs):
            response_parts.append(chunk)
        
        return "".join(response_parts)
    
    # æ›¿æ¢åŸæœ‰çš„ agenerate_stream
    async def agenerate_stream(self, prompt: str, system_prompt: str = None, 
                            history_messages: list = None, **kwargs) -> AsyncIterator[str]:
        full_prompt = prompt
        if system_prompt:
            full_prompt = f"System: {system_prompt}\nUser: {prompt}"
        
        data = {
            "prompt": full_prompt,
            "max_new_tokens": kwargs.get('max_new_tokens', self.config['max_new_tokens'])
        }

        loop = asyncio.get_running_loop()

        try:
            # åœ¨çº¿ç¨‹æ± ä¸­è¿è¡ŒåŒæ­¥çš„ requests ä»£ç ï¼Œé¿å…é˜»å¡äº‹ä»¶å¾ªç¯
            def blocking_request():
                response = requests.post(
                    f"{API_BASE_URL}/generate",
                    json=data,
                    timeout=180,
                    stream=True  # å…³é”®ï¼šå¼€å¯æµå¼è¯·æ±‚
                )
                response.raise_for_status() # å¦‚æœHTTPçŠ¶æ€ç æ˜¯4xx/5xxï¼Œåˆ™æŠ›å‡ºå¼‚å¸¸
                return response.iter_content(chunk_size=None, decode_unicode=True)

            # iter_content æ˜¯ä¸€ä¸ªåŒæ­¥è¿­ä»£å™¨ï¼Œæˆ‘ä»¬éœ€è¦åœ¨å¼‚æ­¥å‡½æ•°ä¸­åŒ…è£…å®ƒ
            sync_iterator = await loop.run_in_executor(None, blocking_request)
            
            for chunk in sync_iterator:
                yield chunk
                await asyncio.sleep(0) # è®©å‡ºæ§åˆ¶æƒï¼Œä¿æŒå¼‚æ­¥ç¯å¢ƒçš„å“åº”æ€§
        
        except requests.exceptions.RequestException as e:
            error_msg = f"Error during API stream request: {e}"
            print(error_msg)
            yield error_msg
        except Exception as e:
            yield f"Error: {str(e)}"

# å…¨å±€é€‚é…å™¨å®ä¾‹
_global_llm_adapter: Optional[QwenLLMAdapter] = None

def get_qwen_llm_adapter(model_path: str = None,
                        config_path: str = None,
                        **kwargs) -> QwenLLMAdapter:
    """è·å–å…¨å±€ LLM é€‚é…å™¨å®ä¾‹"""
    global _global_llm_adapter
    
    if _global_llm_adapter is None:
        config_kwargs = kwargs.copy()
        if model_path:
            config_kwargs["model_path"] = model_path
        if config_path:
            config_kwargs["config_path"] = config_path
            
        _global_llm_adapter = QwenLLMAdapter(**config_kwargs)
    
    return _global_llm_adapter

async def qwen_llm_model_func(prompt: str,
                             system_prompt: str = None,
                             history_messages: List = None,
                             keyword_extraction: bool = False,
                             stream: bool = False,
                             hashing_kv=None,
                             model_path: str = None,
                             config_path: str = None,
                             **kwargs) -> str:
    """LightRAG å…¼å®¹çš„ LLM å‡½æ•°"""
    adapter = get_qwen_llm_adapter(
        model_path=model_path,
        config_path=config_path,
        **kwargs
    )
    
    if stream:
        async def stream_generator():
            async for chunk in adapter.agenerate_stream(
                prompt, system_prompt, history_messages, **kwargs
            ):
                yield chunk
        return stream_generator()
    else:
        return await adapter.agenerate(
            prompt, system_prompt, history_messages, stream, **kwargs
        )

async def test_streaming():
    print("ğŸ§ª æµ‹è¯•å¼‚æ­¥æµå¼ç”Ÿæˆ...")
    adapter = get_qwen_llm_adapter()
    prompt = "ç®€å•ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½"
    print(f"Prompt: {prompt}")
    
    response_stream = adapter.agenerate_stream(prompt)
    async for chunk in response_stream:
        print(chunk, end="", flush=True)
    print("\nâœ… æµå¼æµ‹è¯•å®Œæˆï¼")


if __name__ == "__main__":
           
    print("="*60)
    asyncio.run(test_streaming())