# å¯¼å…¥æ“ä½œç³»ç»Ÿæ¨¡å—ï¼Œç”¨äºæ–‡ä»¶è·¯å¾„å’Œç¯å¢ƒå˜é‡æ“ä½œ
import os
# ç¦ç”¨ tiktoken ç¼“å­˜ç›®å½•ï¼Œé¿å…ç½‘ç»œè¯·æ±‚

os.environ['TOKENIZERS_PARALLELISM'] = 'false'
os.environ['PYTHONUNBUFFERED'] = '1'

import sys
import hashlib
from watchdog.observers.polling import PollingObserver as Observer
from watchdog.events import FileSystemEventHandler

# æ·»åŠ é€‚é…å™¨æ¨¡å—è·¯å¾„
sys.path.append('/data/qwen_embedding')
# æ·»åŠ  Qwen LLM é€‚é…å™¨è·¯å¾„
sys.path.append('/data/whisper-TPU_py/bmwhisper')

# å¯¼å…¥å¼‚æ­¥ç¼–ç¨‹æ¨¡å—
import asyncio
# å¯¼å…¥æ£€æŸ¥æ¨¡å—ï¼Œç”¨äºæ£€æŸ¥å¯¹è±¡ç±»å‹
import inspect
# å¯¼å…¥æ—¥å¿—è®°å½•æ¨¡å—
import logging
# å¯¼å…¥æ—¥å¿—é…ç½®æ¨¡å—
import logging.config
# å¯¼å…¥LightRAGæ ¸å¿ƒç»„ä»¶ï¼šä¸»ç±»å’ŒæŸ¥è¯¢å‚æ•°ç±»
from lightrag import LightRAG, QueryParam
# å¯¼å…¥OpenAIå…¼å®¹çš„å®Œæˆå‡½æ•°
from lightrag.llm.openai import openai_complete_if_cache
# å¯¼å…¥å·¥å…·å‡½æ•°ï¼šåµŒå…¥å‡½æ•°ç±»ã€æ—¥å¿—å™¨ã€è¯¦ç»†è°ƒè¯•è®¾ç½®
from lightrag.utils import EmbeddingFunc, logger, set_verbose_debug
# å¯¼å…¥å…±äº«å­˜å‚¨çš„ç®¡é“çŠ¶æ€åˆå§‹åŒ–å‡½æ•°
from lightrag.kg.shared_storage import initialize_pipeline_status
# å¯¼å…¥PyTorchæ·±åº¦å­¦ä¹ æ¡†æ¶
import torch
import numpy as np
import time

# å¯¼å…¥æˆ‘ä»¬çš„ LightRAG Qwen é€‚é…å™¨ï¼ˆç”¨äºembeddingï¼‰
from lightrag_qwen_adapter import get_lightrag_embedding_func

# å¯¼å…¥ Qwen LLM é€‚é…å™¨ï¼ˆç”¨äº LLMï¼Œé€šè¿‡ HTTP APIï¼‰
from qwen_llm_adapter import qwen_llm_model_func, MODEL_CONFIG

# å¯¼å…¥ç®€åŒ–çš„å…±äº«ç®¡ç†å™¨ç”¨äºçŠ¶æ€æ£€æŸ¥
from shared_qwen_manager import simple_qwen

# ä¿®æ”¹æ¨¡å‹è·¯å¾„
MODEL_CONFIG["llm_model_path"] = "/data/qwen4btune_w4bf16_seq8192_bm1684x_1dev_20250721_195513.bmodel"
MODEL_CONFIG["config_path"] = "/data/LLM-TPU/models/Qwen3/python_demo/config"
MODEL_CONFIG["temperature"] = 0.5

# å®šä¹‰å·¥ä½œç›®å½•è·¯å¾„
WORKING_DIR = "./result/biography_final"
# åœ¨ WORKING_DIR å®šä¹‰åæ·»åŠ è¾“å‡ºç›®å½•é…ç½®
OUTPUT_DIR = "/data/mermaidRender/dist"

# ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
if not os.path.exists(OUTPUT_DIR):
    print(f"Creating output directory: {OUTPUT_DIR}")
    os.makedirs(OUTPUT_DIR)
else:
    print(f"Output directory already exists: {OUTPUT_DIR},continue...")
    
# å¦‚æœå·¥ä½œç›®å½•ä¸å­˜åœ¨ï¼Œåˆ™åˆ›å»ºè¯¥ç›®å½•
if not os.path.exists(WORKING_DIR):
    print(f"Setting directory not exist,creating working directory: {WORKING_DIR}")
    os.mkdir(WORKING_DIR)
else:
    print(f"Working directory already exists: {WORKING_DIR},continue...")

# æ–‡ä»¶ç›‘æ§å¤„ç†å™¨ç±» - ä¿®æ”¹ç‰ˆæœ¬
class QuestionFileHandler(FileSystemEventHandler):
    """é—®é¢˜æ–‡ä»¶å˜åŒ–ç›‘æ§å¤„ç†å™¨"""
    def __init__(self, callback, loop):
        self.callback = callback
        self.last_content_hash = None
        self.loop = loop  # ä¿å­˜äº‹ä»¶å¾ªç¯å¼•ç”¨
        
    def on_modified(self, event):
        if not event.is_directory and event.src_path.endswith('ragQuestions.txt'):
            # æ£€æŸ¥æ–‡ä»¶å†…å®¹æ˜¯å¦çœŸçš„æ”¹å˜äº†
            try:
                with open(event.src_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                content_hash = hashlib.md5(content.encode()).hexdigest()
                
                if content_hash != self.last_content_hash:
                    self.last_content_hash = content_hash
                    # ä½¿ç”¨ run_coroutine_threadsafe å®‰å…¨åœ°åœ¨äº‹ä»¶å¾ªç¯ä¸­è°ƒåº¦åç¨‹
                    asyncio.run_coroutine_threadsafe(
                        self.callback(event.src_path), 
                        self.loop
                    )
            except Exception as e:
                print(f"è¯»å–æ–‡ä»¶æ—¶å‡ºé”™: {e}")

# å®šä¹‰æ—¥å¿—é…ç½®å‡½æ•°
def configure_logging():
    """é…ç½®åº”ç”¨ç¨‹åºçš„æ—¥å¿—è®°å½•"""
    # é‡ç½®ç°æœ‰çš„å¤„ç†ç¨‹åºä»¥ç¡®ä¿å¹²å‡€çš„é…ç½®
    for logger_name in ["uvicorn", "uvicorn.access", "uvicorn.error", "lightrag"]:
        logger_instance = logging.getLogger(logger_name)
        logger_instance.handlers = []
        logger_instance.filters = []

    # ä»ç¯å¢ƒå˜é‡è·å–æ—¥å¿—ç›®å½•è·¯å¾„ï¼Œé»˜è®¤ä½¿ç”¨å½“å‰ç›®å½•
    log_dir = os.getenv("LOG_DIR", os.getcwd())
    # æ„å»ºæ—¥å¿—æ–‡ä»¶çš„ç»å¯¹è·¯å¾„
    log_file_path = os.path.abspath(
        os.path.join(log_dir, "lightrag_qwen_bmodel_demo.log")
    )

    print(f"\nLightRAG Qwen embedding bmodel demo log file: {log_file_path}\n")
    os.makedirs(os.path.dirname(log_dir), exist_ok=True)

    # ä»ç¯å¢ƒå˜é‡è·å–æ—¥å¿—æ–‡ä»¶æœ€å¤§å¤§å°å’Œå¤‡ä»½æ•°é‡
    log_max_bytes = int(os.getenv("LOG_MAX_BYTES", 10485760))  # é»˜è®¤10MB
    log_backup_count = int(os.getenv("LOG_BACKUP_COUNT", 5))  # é»˜è®¤5ä¸ªå¤‡ä»½

    # é…ç½®æ—¥å¿—ç³»ç»Ÿ
    logging.config.dictConfig(
        {
            "version": 1,
            "disable_existing_loggers": False,
            "formatters": {
                "default": {
                    "format": "%(levelname)s: %(message)s",
                },
                "detailed": {
                    "format": "%(asctime)s - %(name)s - %(levelname)s - %(message)s",
                },
            },
            "handlers": {
                "console": {
                    "formatter": "default",
                    "class": "logging.StreamHandler",
                    "stream": "ext://sys.stderr",
                },
                "file": {
                    "formatter": "detailed",
                    "class": "logging.handlers.RotatingFileHandler",
                    "filename": log_file_path,
                    "maxBytes": log_max_bytes,
                    "backupCount": log_backup_count,
                    "encoding": "utf-8",
                },
            },
            "loggers": {
                "lightrag": {
                    "handlers": ["console", "file"],
                    "level": "INFO",
                    "propagate": False,
                },
            },
        }
    )

    logger.setLevel(logging.INFO)
    set_verbose_debug(os.getenv("VERBOSE_DEBUG", "false").lower() == "true")

# å®šä¹‰æµå¼è¾“å‡ºæ‰“å°å‡½æ•°
async def print_stream(stream):
    async for chunk in stream:
        if chunk:
            print(chunk, end="", flush=True)

# æ£€æŸ¥æ¨¡å‹çŠ¶æ€çš„å‡½æ•°
def check_model_availability():
    """æ£€æŸ¥æ¨¡å‹æ˜¯å¦å¯ç”¨"""
    try:
        status = simple_qwen.get_status()
        print(f"æ¨¡å‹çŠ¶æ€æ£€æŸ¥: {status}")
        
        if status["local_model"] or status["external_model"]:
            print("âœ… æ£€æµ‹åˆ°å¯ç”¨çš„ LLM æ¨¡å‹")
            return True
        else:
            print("âš ï¸ æœªæ£€æµ‹åˆ°å¯ç”¨çš„ LLM æ¨¡å‹")
            return False
    except Exception as e:
        print(f"âŒ æ¨¡å‹çŠ¶æ€æ£€æŸ¥å¤±è´¥: {e}")
        return False

# æ£€æŸ¥ API æœåŠ¡å™¨çŠ¶æ€
def check_api_server_status():
    """æ£€æŸ¥APIæœåŠ¡å™¨çŠ¶æ€"""
    try:
        import requests
        response = requests.get("http://localhost:8899/status", timeout=5)
        if response.status_code == 200:
            status = response.json()
            print(f"âœ… APIæœåŠ¡å™¨å¯ç”¨: {status}")
            return True
        else:
            print(f"âš ï¸ APIæœåŠ¡å™¨å“åº”å¼‚å¸¸: {response.status_code}")
            return False
    except Exception as e:
        print(f"âŒ APIæœåŠ¡å™¨ä¸å¯ç”¨: {e}")
        return False

async def initialize_rag():
    """ä½¿ç”¨ Qwen embedding bmodel + HTTP LLM API åˆå§‹åŒ– RAG ç³»ç»Ÿ"""
    
    print("æ­£åœ¨åˆå§‹åŒ– LightRAG with Qwen embedding bmodel + HTTP LLM API...")
    
    # 1. æ£€æŸ¥ LLM æ¨¡å‹çŠ¶æ€
    print("\n=== æ£€æŸ¥ LLM æ¨¡å‹çŠ¶æ€ ===")
    llm_available = check_model_availability()
    api_available = check_api_server_status()
    
    if not (llm_available or api_available):
        print("âš ï¸ è­¦å‘Šï¼šLLM æ¨¡å‹å’ŒAPIæœåŠ¡å™¨éƒ½ä¸å¯ç”¨")
        print("è¯·ç¡®ä¿ sample_audio.py æ­£åœ¨è¿è¡Œ")
        print("ç»§ç»­åˆå§‹åŒ–ï¼Œä½†LLMåŠŸèƒ½å¯èƒ½å—é™...")
    else:
        print("âœ… LLM æ¨¡å‹é€šè¿‡ HTTP API å¯ç”¨")
    
    # 2. åˆå§‹åŒ– Embedding æ¨¡å‹ï¼ˆæœ¬åœ°åŠ è½½ï¼‰
    print("\n=== åˆå§‹åŒ– Embedding æ¨¡å‹ ===")
    try:
        qwen_embedding_func = get_lightrag_embedding_func(
            model_path='/data/Qwen3_Embedding_0.6B_my_1684x_128_f16.bmodel',
            tokenizer_path=None,
            batch_size=1,
            device="tpu",
            async_mode=True
        )
        print("âœ… Embedding æ¨¡å‹åˆå§‹åŒ–æˆåŠŸï¼ˆæœ¬åœ°bmodelï¼‰")
    except Exception as e:
        print(f"âŒ Embedding æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
        raise
    
    # 3. åˆ›å»º LightRAG å®ä¾‹
    print("\n=== åˆ›å»º LightRAG å®ä¾‹ ===")
    try:
        # åˆ›å»ºè‡ªå®šä¹‰ tokenizer æ¥æ›¿ä»£ tiktoken
        class SimpleChineseTokenizer:
            """ç®€å•çš„ä¸­æ–‡åˆ†è¯å™¨ï¼Œé¿å…ä½¿ç”¨ tiktoken"""
            
            def encode(self, text: str) -> list[int]:
                """å°†æ–‡æœ¬ç¼–ç ä¸º token ID åˆ—è¡¨"""
                # å­—ç¬¦çº§ç¼–ç ï¼Œé€‚åˆä¸­æ–‡
                return [ord(c) for c in text if ord(c) <= 65535]  # è¿‡æ»¤è¶…å‡ºBMPçš„å­—ç¬¦
            
            def decode(self, tokens: list[int]) -> str:
                """å°† token ID åˆ—è¡¨è§£ç ä¸ºæ–‡æœ¬"""
                try:
                    return ''.join([chr(t) for t in tokens if 0 <= t <= 65535])
                except ValueError:
                    return ""
        
        # åˆ›å»º tokenizer å®ä¾‹
        from lightrag.utils import Tokenizer
        custom_tokenizer = Tokenizer(
            model_name="chinese_custom", 
            tokenizer=SimpleChineseTokenizer()
        )
        
        print("âœ… è‡ªå®šä¹‰ tokenizer åˆ›å»ºæˆåŠŸï¼ˆæ— éœ€ tiktokenï¼‰")
        
        # åˆ›å»ºLightRAGå®ä¾‹ï¼Œä½¿ç”¨HTTP APIçš„LLM + æœ¬åœ°embedding + è‡ªå®šä¹‰tokenizer
        rag = LightRAG(
            working_dir=WORKING_DIR,
            llm_model_func=qwen_llm_model_func,  # ä½¿ç”¨HTTP API
            embedding_func=EmbeddingFunc(
                embedding_dim=1024,
                max_token_size=5000,
                func=qwen_embedding_func,  # ä½¿ç”¨æœ¬åœ°bmodel
            ),
            tokenizer=custom_tokenizer,  # å…³é”®ï¼šä½¿ç”¨è‡ªå®šä¹‰tokenizer
            tiktoken_model_name=None,     # ç¡®ä¿ä¸ä½¿ç”¨tiktoken
        )
        print("âœ… LightRAG å®ä¾‹åˆ›å»ºæˆåŠŸ")
        
    except Exception as e:
        print(f"âŒ LightRAG åˆå§‹åŒ–å¤±è´¥: {e}")
        print("å°è¯•ä½¿ç”¨æœ€å°é…ç½®é‡æ–°åˆå§‹åŒ–...")
        
        # å¤‡ç”¨æ–¹æ¡ˆï¼šæ›´ç®€å•çš„tokenizer
        class BasicTokenizer:
            def encode(self, text: str) -> list[int]:
                # æœ€ç®€å•çš„å­—èŠ‚ç¼–ç 
                return list(text.encode('utf-8'))
            
            def decode(self, tokens: list[int]) -> str:
                try:
                    return bytes(tokens).decode('utf-8', errors='ignore')
                except:
                    return ""
        
        from lightrag.utils import Tokenizer
        basic_tokenizer = Tokenizer(
            model_name="basic", 
            tokenizer=BasicTokenizer()
        )
        
        rag = LightRAG(
            working_dir=WORKING_DIR,
            llm_model_func=qwen_llm_model_func,
            embedding_func=EmbeddingFunc(
                embedding_dim=1024,
                max_token_size=5000,
                func=qwen_embedding_func,
            ),
            tokenizer=basic_tokenizer,    # ä½¿ç”¨åŸºç¡€tokenizer
            tiktoken_model_name=None,
        )

    # 4. åˆå§‹åŒ–å­˜å‚¨ç³»ç»Ÿ
    print("\n=== åˆå§‹åŒ–å­˜å‚¨ç³»ç»Ÿ ===")
    try:
        await rag.initialize_storages()
        await initialize_pipeline_status()
        print("âœ… å­˜å‚¨ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    except Exception as e:
        print(f"âš ï¸ å­˜å‚¨ç³»ç»Ÿåˆå§‹åŒ–å‡ºç°é—®é¢˜: {e}")
        print("ç»§ç»­è¿è¡Œ...")

    print("âœ… LightRAG åˆå§‹åŒ–å®Œæˆï¼ˆEmbeddingæœ¬åœ° + LLMè¿œç¨‹APIï¼‰")
    return rag

# é—®é¢˜æ–‡ä»¶è§£æå‡½æ•°
def parse_question_file(file_path: str) -> tuple:
    """è§£æé—®é¢˜æ–‡ä»¶ï¼Œè¿”å› (åºå·, é—®é¢˜å†…å®¹)"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            lines = [line.strip() for line in f.readlines() if line.strip()]
        
        if len(lines) >= 2:
            sequence_num = lines[0]
            question = lines[1]
            return sequence_num, question
        elif len(lines) == 1:
            # åªæœ‰ä¸€è¡Œï¼Œå½“ä½œé—®é¢˜å¤„ç†
            return "1", lines[0]
        else:
            return None, None
            
    except FileNotFoundError:
        print(f"é—®é¢˜æ–‡ä»¶ {file_path} ä¸å­˜åœ¨")
        return None, None
    except Exception as e:
        print(f"è§£æé—®é¢˜æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return None, None

# æ–‡æ¡£æ¥å£å‡½æ•°
def read_questions_from_file(file_path: str) -> list:
    """ä»æ–‡ä»¶ä¸­è¯»å–é—®é¢˜åˆ—è¡¨"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            questions = [line.strip() for line in f if line.strip()]
        return questions
    except FileNotFoundError:
        print(f"Warning: {file_path} not found, using default question")
        return ["ä»‹ç»ç™½è¡€ç—…ä¸ç™½ç»†èƒçš„å…³ç³»"]  # é»˜è®¤é—®é¢˜

def save_chunk_ids(chunk_ids: list, file_path: str):
    """å°†chunk IDä¿å­˜åˆ°æ–‡ä»¶"""
    with open(file_path, 'w', encoding='utf-8') as f:
        for chunk_id in chunk_ids:
            f.write(f"{chunk_id}\n")
    print(f"Chunk IDs saved to: {file_path}")

# å¤„ç†å•ä¸ªé—®é¢˜çš„å‡½æ•°
async def process_single_question(rag, question: str, sequence_num: str):
    """å¤„ç†å•ä¸ªé—®é¢˜çš„å®Œæ•´æµç¨‹"""
    print(f"\n{'='*60}")
    print(f"å¤„ç†é—®é¢˜åºå· {sequence_num}: {question}")
    print('='*60)
    
    try:
        # 1. ç›´æ¥è°ƒç”¨ chunks_vdb.query æ¥è·å–æ£€ç´¢ç»“æœ
        print("\n=====================")
        print("Direct chunk search")
        print("=====================")
        start_time = time.time()
        
        try:
            chunks_result = await rag.chunks_vdb.query(
                query=question,
                top_k=5,
                ids=None
            )
            
            # æå– chunk IDs
            chunk_ids = [chunk.get('id', 'N/A') for chunk in chunks_result]
            
            # æ‰“å°æŸ¥è¯¢ç»“æœ
            print(f"æ£€ç´¢åˆ° {len(chunks_result)} ä¸ªç›¸å…³ chunks:")
            for i, chunk in enumerate(chunks_result):
                print(f"\nChunk {i+1}:")
                print(f"  ID: {chunk.get('id', 'N/A')}")
                print(f"  Score: {chunk.get('distance', 'N/A')}")
                print(f"  Content: {chunk.get('content', '')[:100]}...")
                print(f"  Full Doc ID: {chunk.get('full_doc_id', 'N/A')}")
                print(f"  Chunk Order Index: {chunk.get('chunk_order_index', 'N/A')}")
                print(f"  File Path: {chunk.get('file_path', 'N/A')}")

            # 2. ä¿å­˜ chunk IDs åˆ° ragSearch.txt
            chunk_ids_file = os.path.join(OUTPUT_DIR, "ragSearch.txt")
            save_chunk_ids(chunk_ids, chunk_ids_file)
            
        except Exception as e:
            print(f"âš ï¸ Chunk æ£€ç´¢å¤±è´¥: {e}")
            chunk_ids_file = os.path.join(OUTPUT_DIR, "ragSearch.txt")
            save_chunk_ids(["æ£€ç´¢å¤±è´¥"], chunk_ids_file)
        
        # 3. æ‰§è¡Œå®Œæ•´çš„RAGæŸ¥è¯¢
        print(f"\n=====================")
        print("Query mode: naive - Full RAG Search")
        print("=====================")
        start_time = time.time()
        
        try:
            resp = await rag.aquery(
                question,
                param=QueryParam(mode="naive", stream=True),
            )
            end_time = time.time()
            print(f"Search execution time: {end_time - start_time:.2f} seconds")
            
            # æ›¿æ¢åŸæœ‰çš„ if inspect.isasyncgen(resp): ... else: ... é€»è¾‘
            response_file = os.path.join(OUTPUT_DIR, "RAGResult.txt")

            # ç¡®ä¿æ–‡ä»¶åœ¨ä½¿ç”¨å‰æ˜¯ç©ºçš„
            with open(response_file, 'w', encoding='utf-8') as f:
                f.write("") # æ¸…ç©ºæ–‡ä»¶

            print("Response lightrag_bmodel2 success:")
            # ä»¥è¿½åŠ æ¨¡å¼æ‰“å¼€æ–‡ä»¶ï¼Œåœ¨å¾ªç¯ä¸­å†™å…¥
            with open(response_file, 'a', encoding='utf-8') as f:
                if inspect.isasyncgen(resp):
                    # æµå¼å“åº”
                    async for chunk in resp:
                        if chunk:
                            # åŒæ—¶è¾“å‡ºåˆ°ç»ˆç«¯å’Œæ–‡ä»¶
                            print(chunk, end="", flush=True)
                            f.write(chunk)
                            f.flush() # å…³é”®ï¼šç¡®ä¿ç«‹å³å†™å…¥ç£ç›˜
                else:
                    # éæµå¼å“åº”ï¼ˆä½œä¸ºå¤‡ç”¨é€»è¾‘ï¼‰
                    response_text = str(resp)
                    print(response_text)
                    f.write(response_text)

            # å¾ªç¯ç»“æŸåæ¢è¡Œï¼Œç¾åŒ–ç»ˆç«¯è¾“å‡º
            print() 

            print(f"\né—®é¢˜åºå· {sequence_num} å¤„ç†å®Œæˆ")
            print(f"- Chunk IDs ä¿å­˜è‡³: {chunk_ids_file}")
            print(f"- å“åº”æµå¼ä¿å­˜è‡³: {response_file}")

            return True
            
        except Exception as e:
            print(f"âš ï¸ RAG æŸ¥è¯¢å¤±è´¥: {e}")
            # å°è¯•ç›´æ¥ä½¿ç”¨ LLM
            print("å°è¯•ç›´æ¥ä½¿ç”¨ LLM å›ç­”...")
            
            return False
        
    except Exception as e:
        print(f"å¤„ç†é—®é¢˜æ—¶å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

        return False

# ç›‘æ§å’Œå¤„ç†é—®é¢˜çš„ä¸»å‡½æ•° - ä¿®æ”¹ç‰ˆæœ¬
async def monitor_and_process_questions(rag):
    """ç›‘æ§é—®é¢˜æ–‡ä»¶å¹¶å¤„ç†"""
    questions_file = os.path.join(OUTPUT_DIR, "ragQuestions.txt")
    processed_sequences = set()  # è®°å½•å·²å¤„ç†çš„åºå·
    
    async def handle_file_change(file_path):
        """å¤„ç†æ–‡ä»¶å˜åŒ–"""
        try:
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ä¸ºç©º
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read().strip()
            
            if not content:
                print("é—®é¢˜æ–‡ä»¶ä¸ºç©ºï¼Œç­‰å¾…ç”¨æˆ·è¾“å…¥é—®é¢˜...")
                return
            
            sequence_num, question = parse_question_file(file_path)
            
            if sequence_num and question:
                # æ£€æŸ¥æ˜¯å¦æ˜¯æ–°é—®é¢˜ï¼ˆåŸºäºåºå·ï¼‰
                if sequence_num not in processed_sequences:
                    print(f"\næ£€æµ‹åˆ°æ–°é—®é¢˜: åºå· {sequence_num}")
                    success = await process_single_question(rag, question, sequence_num)
                    
                    if success:
                        processed_sequences.add(sequence_num)
                        print(f"é—®é¢˜åºå· {sequence_num} å¤„ç†æˆåŠŸ")
                    else:
                        print(f"é—®é¢˜åºå· {sequence_num} å¤„ç†å¤±è´¥")
                else:
                    print(f"é—®é¢˜åºå· {sequence_num} å·²å¤„ç†è¿‡ï¼Œè·³è¿‡")
            else:
                print("é—®é¢˜æ–‡ä»¶æ ¼å¼ä¸æ­£ç¡®æˆ–ä¸ºç©º")
        except Exception as e:
            print(f"å¤„ç†æ–‡ä»¶å˜åŒ–æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
    
    # è·å–å½“å‰äº‹ä»¶å¾ªç¯
    loop = asyncio.get_running_loop()
    
    # åˆ›å»ºæ–‡ä»¶ç›‘æ§å¤„ç†å™¨ï¼Œä¼ å…¥äº‹ä»¶å¾ªç¯
    event_handler = QuestionFileHandler(callback=handle_file_change, loop=loop)
    
    # è®¾ç½®æ–‡ä»¶ç›‘æ§
    observer = Observer()
    observer.schedule(event_handler, OUTPUT_DIR, recursive=False)
    observer.start()
    
    print(f"å¼€å§‹ç›‘æ§é—®é¢˜æ–‡ä»¶: {questions_file}")
    print("è¯·åœ¨ ragQuestions.txt ä¸­å†™å…¥é—®é¢˜ï¼Œæ ¼å¼å¦‚ä¸‹:")
    print("ç¬¬ä¸€è¡Œ: åºå·")
    print("ç¬¬äºŒè¡Œ: é—®é¢˜å†…å®¹")
    print("æŒ‰ Ctrl+C é€€å‡ºç›‘æ§")
    
    # åˆå§‹åŒ–æ—¶è¯»å–å½“å‰åºå·ï¼Œä½†ä¸å¤„ç†
    if os.path.exists(questions_file):
        try:
            sequence_num, _ = parse_question_file(questions_file)
            if sequence_num:
                processed_sequences.add(sequence_num)
                print(f"æ£€æµ‹åˆ°ç°æœ‰é—®é¢˜æ–‡ä»¶ï¼Œå½“å‰åºå·: {sequence_num}ï¼Œç­‰å¾…åºå·å˜åŒ–...")
        except:
            pass
    
    try:
        # ä¿æŒç›‘æ§è¿è¡Œ
        while True:
            await asyncio.sleep(1)
    except KeyboardInterrupt:
        print("\nåœæ­¢æ–‡ä»¶ç›‘æ§")
        observer.stop()
    
    observer.join()

def create_example_question_file():
    """åˆ›å»ºç©ºçš„é—®é¢˜æ–‡ä»¶"""
    questions_file = os.path.join(OUTPUT_DIR, "ragQuestions.txt")
    if not os.path.exists(questions_file):
        # åˆ›å»ºç©ºæ–‡ä»¶ï¼Œä¸å†™å…¥ä»»ä½•å†…å®¹
        with open(questions_file, 'w', encoding='utf-8') as f:
            f.write("")
        
        print(f"åˆ›å»ºç©ºçš„é—®é¢˜æ–‡ä»¶: {questions_file}")
        print("ç­‰å¾…ç”¨æˆ·è¾“å…¥é—®é¢˜...")
    else:
        print(f"é—®é¢˜æ–‡ä»¶å·²å­˜åœ¨: {questions_file}")
        print("ç­‰å¾…é—®é¢˜åºå·å˜åŒ–...")

# å®šä¹‰å¼‚æ­¥ä¸»å‡½æ•°
async def main():
    try:
        # é¦–å…ˆæ£€æŸ¥æ¨¡å‹çŠ¶æ€
        print("=" * 60)
        print("æ£€æŸ¥ç³»ç»ŸçŠ¶æ€...")
        print("=" * 60)
        
        llm_available = check_model_availability()
        api_available = check_api_server_status()
        
        if not (llm_available or api_available):
            print("âš ï¸ è­¦å‘Šï¼šLLMæ¨¡å‹å¯èƒ½æœªå‡†å¤‡å°±ç»ª")
            print("è¯·ç¡®ä¿ sample_audio.py æ­£åœ¨è¿è¡Œ")
            print("ç»§ç»­åˆå§‹åŒ–ï¼Œå°†å°è¯•é€šè¿‡APIè®¿é—®...")
        
        # åˆå§‹åŒ–RAGå®ä¾‹
        print("\n" + "=" * 60)
        print("åˆå§‹åŒ– RAG ç³»ç»Ÿ...")
        print("=" * 60)
        
        rag = await initialize_rag()

        # æµ‹è¯•åµŒå…¥å‡½æ•°
        test_text = ["This is a test string for embedding with Qwen embedding bmodel."]
        print("\n=======================")
        print("Testing Qwen embedding bmodel embedding function")
        print("========================")
        print(f"Test text: {test_text}")
        
        try:
            embedding = await rag.embedding_func(test_text)
            embedding_dim = embedding.shape[1]
            print(f"âœ… Detected embedding dimension: {embedding_dim}")
            print(f"Embedding shape: {embedding.shape}")
            print(f"First 10 values: {embedding[0][:10]}\n")
        except Exception as e:
            print(f"âš ï¸ Embedding æµ‹è¯•å¤±è´¥: {e}")

        # # æµ‹è¯•LLMå‡½æ•°
        # print("\n=======================")
        # print("Testing Qwen LLM HTTP API function")
        # print("========================")
        # test_prompt = "ä½ å¥½ï¼Œè¯·ç®€å•ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚"
        # print(f"Test prompt: {test_prompt}")
        
        # try:
        #     llm_response = await rag.llm_model_func(
        #         test_prompt,
        #         system_prompt="ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹ã€‚"
        #     )
        #     print(f"âœ… LLM Response: {llm_response}\n")
        # except Exception as e:
        #     print(f"âš ï¸ LLM æµ‹è¯•å¤±è´¥: {e}")
        #     print("å¯èƒ½åŸå› ï¼šsample_audio.py æœªè¿è¡Œæˆ–APIæœåŠ¡å™¨æœªå¯åŠ¨")
        
        # åˆ›å»ºç¤ºä¾‹é—®é¢˜æ–‡ä»¶
        create_example_question_file()
        
        # å¼€å§‹ç›‘æ§å’Œå¤„ç†é—®é¢˜
        print("\n" + "=" * 60)
        print("å¼€å§‹é—®é¢˜ç›‘æ§...")
        print("=" * 60)
        
        await monitor_and_process_questions(rag)
        
    except Exception as e:
        print(f"An error occurred: {e}")
        import traceback
        traceback.print_exc()
    finally:
        if 'rag' in locals():
            try:
                await rag.finalize_storages()
            except:
                pass

# ç¨‹åºå…¥å£ç‚¹
if __name__ == "__main__":
    # åœ¨è¿è¡Œä¸»å‡½æ•°å‰é…ç½®æ—¥å¿—
    configure_logging()
    
    print("ğŸš€ å¯åŠ¨ LightRAG with Qwen bmodel (æ··åˆæ¨¡å¼)")
    print("=" * 60)
    print("æ¶æ„è¯´æ˜:")
    print("- LLM: é€šè¿‡ HTTP API è®¿é—® (qwen_llm_adapter.py)")
    print("- Embedding: ç›´æ¥åŠ è½½æœ¬åœ° bmodel")
    print("- æ³¨æ„ï¼šè¯·ç¡®ä¿ sample_audio.py æ­£åœ¨è¿è¡Œä»¥æä¾›LLMæœåŠ¡")
    print("=" * 60)
    
    # è¿è¡Œå¼‚æ­¥ä¸»å‡½æ•°
    asyncio.run(main())
    
    # æ‰“å°å®Œæˆä¿¡æ¯
    print("\nDone! Qwen embedding bmodel integration with LightRAG completed successfully.")