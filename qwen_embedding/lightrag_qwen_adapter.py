"""
LightRAG Qwen embedding bmodel é€‚é…å™¨
ä¸º qwen_embedding.py æä¾› LightRAG å…¼å®¹çš„æ¥å£
ä¿æŒåŸå§‹æ–‡ä»¶ä¸å˜ï¼Œä»…é€šè¿‡é€‚é…å™¨å®ç°é›†æˆ
"""

import sys
import os
import numpy as np
import torch
from typing import List, Optional
import asyncio
from functools import wraps

try:
    from qwen_embedding import load_model, QwenEmbedding
except ImportError as e:
    print(f"Error importing qwen_embedding: {e}")
    print(f"Please ensure qwen_embedding.py is available in the current directory")
    raise

class LightRAGQwenAdapter:
    """
    LightRAG å…¼å®¹çš„ Qwen embedding bmodel é€‚é…å™¨
    
    è¿™ä¸ªç±»å°† qwen_embedding.py ä¸­çš„ QwenEmbedding åŒ…è£…æˆ LightRAG æœŸæœ›çš„æ ¼å¼
    """
    
    def __init__(self, 
                 model_path: Optional[str] = None,
                 tokenizer_path: Optional[str] = None,
                 batch_size: int = 1,
                 device: str = "tpu"):
        """
        åˆå§‹åŒ–é€‚é…å™¨
        
        å‚æ•°:
        model_path: bmodel æ–‡ä»¶è·¯å¾„
        tokenizer_path: åˆ†è¯å™¨æ–‡ä»¶è·¯å¾„  
        batch_size: æ‰¹å¤„ç†å¤§å°
        device: è®¾å¤‡ç±»å‹
        """
        self.model_path = model_path
        self.tokenizer_path = tokenizer_path
        self.batch_size = batch_size
        self.device = device
        self._model: Optional[QwenEmbedding] = None
        self._model_loaded = False
    
    def _ensure_model_loaded(self):
        """å»¶è¿ŸåŠ è½½æ¨¡å‹ï¼Œé¿å…åˆå§‹åŒ–æ—¶çš„æ€§èƒ½å¼€é”€"""
        if not self._model_loaded:
            print("æ­£åœ¨åŠ è½½ Qwen embedding bmodel...")
            self._model = load_model(
                device=self.device,
                batch_size=self.batch_size,
                model_path=self.model_path,
                tokenizer_path=self.tokenizer_path
            )
            self._model_loaded = True
            print("Qwen embedding bmodel åŠ è½½å®Œæˆ")
    
    def _get_sentence_embedding(self, text: str, token_embeddings: torch.Tensor) -> np.ndarray:
        """
        ä» token embeddings æå–å¥å­çº§åˆ«çš„åµŒå…¥
        
        å‚æ•°:
        text: åŸå§‹æ–‡æœ¬
        token_embeddings: token çº§åˆ«çš„åµŒå…¥ [1, 128, 1024]
        
        è¿”å›:
        np.ndarray: å¥å­çº§åˆ«çš„åµŒå…¥ [1024]
        """
        # è·å– token ids ä»¥è®¡ç®—æ³¨æ„åŠ›æ©ç 
        tokens = self._model.tokenize(text)  # [1, 128]
        attention_mask = (tokens != 0).float()  # [1, 128]
        
        # è®¡ç®—åŠ æƒå¹³å‡ï¼ˆå¿½ç•¥ padding tokenï¼‰
        valid_embeddings = token_embeddings * attention_mask.unsqueeze(-1)  # [1, 128, 1024]
        
        # è®¡ç®—å¹³å‡å€¼
        valid_token_count = attention_mask.sum(dim=1, keepdim=True)  # [1, 1]
        sentence_embedding = valid_embeddings.sum(dim=1) / valid_token_count  # [1, 1024]
        
        return sentence_embedding.squeeze(0).numpy()  # [1024]
    
    def embed_texts(self, texts: List[str]) -> np.ndarray:
        """
        ä¸ºæ–‡æœ¬åˆ—è¡¨ç”ŸæˆåµŒå…¥å‘é‡ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰
        
        å‚æ•°:
        texts: æ–‡æœ¬åˆ—è¡¨
        
        è¿”å›:
        np.ndarray: åµŒå…¥çŸ©é˜µï¼Œå½¢çŠ¶ä¸º [batch_size, embedding_dim]
        """
        self._ensure_model_loaded()
        
        embeddings = []
        
        for text in texts:
            # ä½¿ç”¨åŸå§‹æ¨¡å‹è¿›è¡Œç¼–ç 
            token_embeddings = self._model.encode_text(text)  # [1, 128, 1024]
            
            # è·å–å¥å­çº§åˆ«çš„åµŒå…¥ï¼ˆä½¿ç”¨å¹³å‡æ± åŒ–ï¼‰
            sentence_embedding = self._get_sentence_embedding(text, token_embeddings)
            embeddings.append(sentence_embedding)
        
        return np.array(embeddings)  # [batch_size, 1024]
    
    async def aembed_texts(self, texts: List[str]) -> np.ndarray:
        """
        å¼‚æ­¥ç‰ˆæœ¬çš„æ–‡æœ¬åµŒå…¥å‡½æ•°
        
        å‚æ•°:
        texts: æ–‡æœ¬åˆ—è¡¨
        
        è¿”å›:
        np.ndarray: åµŒå…¥çŸ©é˜µ
        """
        # åœ¨äº‹ä»¶å¾ªç¯ä¸­è¿è¡ŒåŒæ­¥å‡½æ•°
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(None, self.embed_texts, texts)


# å…¨å±€é€‚é…å™¨å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰
_global_adapter: Optional[LightRAGQwenAdapter] = None


def get_qwen_adapter(model_path: Optional[str] = None,
                    tokenizer_path: Optional[str] = None,
                    batch_size: int = 1,
                    device: str = "tpu") -> LightRAGQwenAdapter:
    """
    è·å–å…¨å±€é€‚é…å™¨å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰
    
    å‚æ•°:
    model_path: bmodel æ–‡ä»¶è·¯å¾„
    tokenizer_path: åˆ†è¯å™¨æ–‡ä»¶è·¯å¾„
    batch_size: æ‰¹å¤„ç†å¤§å°
    device: è®¾å¤‡ç±»å‹
    
    è¿”å›:
    LightRAGQwenAdapter: é€‚é…å™¨å®ä¾‹
    """
    global _global_adapter
    
    if _global_adapter is None:
        _global_adapter = LightRAGQwenAdapter(
            model_path=model_path,
            tokenizer_path=tokenizer_path,
            batch_size=batch_size,
            device=device
        )
    
    return _global_adapter


# ä¸º LightRAG æä¾›çš„ä¸»è¦æ¥å£å‡½æ•°
def get_lightrag_embedding_func(model_path: Optional[str] = None,
                               tokenizer_path: Optional[str] = None,
                               batch_size: int = 1,
                               device: str = "tpu",
                               async_mode: bool = True):  # é»˜è®¤ä¸ºå¼‚æ­¥æ¨¡å¼
    """
    è·å– LightRAG å…¼å®¹çš„åµŒå…¥å‡½æ•°
    
    å‚æ•°:
    model_path: bmodel æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸º None åˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„
    tokenizer_path: åˆ†è¯å™¨æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸º None åˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„
    batch_size: æ‰¹å¤„ç†å¤§å°
    device: è®¾å¤‡ç±»å‹
    async_mode: æ˜¯å¦è¿”å›å¼‚æ­¥ç‰ˆæœ¬çš„å‡½æ•°
    
    è¿”å›:
    callable: LightRAG å…¼å®¹çš„åµŒå…¥å‡½æ•°
    """
    if async_mode:
        async def async_embed_func(texts: List[str]) -> np.ndarray:
            adapter = get_qwen_adapter(model_path, tokenizer_path, batch_size, device)
            return await adapter.aembed_texts(texts)
        return async_embed_func
    else:
        def sync_embed_func(texts: List[str]) -> np.ndarray:
            adapter = get_qwen_adapter(model_path, tokenizer_path, batch_size, device)
            return adapter.embed_texts(texts)
        return sync_embed_func


def test_adapter():
    """
    æµ‹è¯•é€‚é…å™¨åŠŸèƒ½
    """
    print("æµ‹è¯• LightRAG Qwen embedding bmodel é€‚é…å™¨...")
    
    # æµ‹è¯•æ–‡æœ¬
    test_texts = [
        "è¿™æ˜¯ç¬¬ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬ï¼Œç”¨äºéªŒè¯ Qwen embedding bmodel çš„åŠŸèƒ½ã€‚",
        "This is the second test text for embedding verification.",
        "æµ‹è¯•ä¸­æ–‡å’Œè‹±æ–‡æ··åˆçš„æ–‡æœ¬ with mixed content for comprehensive testing."
    ]
    
    try:
        # è·å–åµŒå…¥å‡½æ•°ï¼ˆåŒæ­¥ç‰ˆæœ¬ç”¨äºæµ‹è¯•ï¼‰
        embed_func = get_lightrag_embedding_func(async_mode=False)
        
        # æµ‹è¯•åµŒå…¥
        print(f"è¾“å…¥æ–‡æœ¬æ•°é‡: {len(test_texts)}")
        embeddings = embed_func(test_texts)
        
        print(f"åµŒå…¥çŸ©é˜µå½¢çŠ¶: {embeddings.shape}")
        print(f"åµŒå…¥ç»´åº¦: {embeddings.shape[1]}")
        print(f"ç¬¬ä¸€ä¸ªæ–‡æœ¬çš„åµŒå…¥å‰10ä¸ªå€¼: {embeddings[0][:10]}")
        
        # éªŒè¯è¾“å‡ºæ ¼å¼
        assert embeddings.shape[0] == len(test_texts), "æ‰¹æ¬¡å¤§å°ä¸åŒ¹é…"
        assert embeddings.shape[1] == 1024, "åµŒå…¥ç»´åº¦ä¸æ­£ç¡®"
        assert embeddings.dtype == np.float32 or embeddings.dtype == np.float64, "æ•°æ®ç±»å‹ä¸æ­£ç¡®"
        
        print("âœ… é€‚é…å™¨æµ‹è¯•é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"âŒ é€‚é…å™¨æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


async def test_adapter_async():
    """
    æµ‹è¯•é€‚é…å™¨çš„å¼‚æ­¥åŠŸèƒ½
    """
    print("æµ‹è¯• LightRAG Qwen embedding bmodel é€‚é…å™¨ï¼ˆå¼‚æ­¥ç‰ˆæœ¬ï¼‰...")
    
    test_texts = [
        "å¼‚æ­¥æµ‹è¯•æ–‡æœ¬ä¸€",
        "Async test text two",
        "æ··åˆå¼‚æ­¥æµ‹è¯• mixed async test"
    ]
    
    try:
        # è·å–å¼‚æ­¥åµŒå…¥å‡½æ•°
        async_embed_func = get_lightrag_embedding_func(async_mode=True)
        
        # æµ‹è¯•å¼‚æ­¥åµŒå…¥
        embeddings = await async_embed_func(test_texts)
        
        print(f"å¼‚æ­¥åµŒå…¥çŸ©é˜µå½¢çŠ¶: {embeddings.shape}")
        print(f"å¼‚æ­¥åµŒå…¥ç»´åº¦: {embeddings.shape[1]}")
        
        print("âœ… å¼‚æ­¥é€‚é…å™¨æµ‹è¯•é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"âŒ å¼‚æ­¥é€‚é…å™¨æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    # è¿è¡ŒåŒæ­¥æµ‹è¯•
    sync_success = test_adapter()
    
    # è¿è¡Œå¼‚æ­¥æµ‹è¯•
    async_success = asyncio.run(test_adapter_async())
    
    if sync_success and async_success:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼é€‚é…å™¨å¯ä»¥ä¸ LightRAG é›†æˆä½¿ç”¨ã€‚")
    else:
        print("\nâŒ æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®å’Œä¾èµ–ã€‚")