'''lightarg_bmodel2.py
# å¯¼å…¥æ“ä½œç³»ç»Ÿæ¨¡å—ï¼Œç”¨äºæ–‡ä»¶è·¯å¾„å’Œç¯å¢ƒå˜é‡æ“ä½œ
import os
# ç¦ç”¨ tiktoken ç¼“å­˜ç›®å½•ï¼Œé¿å…ç½‘ç»œè¯·æ±‚

os.environ['TOKENIZERS_PARALLELISM'] = 'false'
os.environ['PYTHONUNBUFFERED'] = '1'

import sys
import hashlib
from watchdog.observers.polling import PollingObserver as Observer
from watchdog.events import FileSystemEventHandler

# æ·»åŠ é€‚é…å™¨æ¨¡å—è·¯å¾„
sys.path.append('/data/qwen_embedding')
# æ·»åŠ  Qwen LLM é€‚é…å™¨è·¯å¾„
sys.path.append('/data/whisper-TPU_py/bmwhisper')

# å¯¼å…¥å¼‚æ­¥ç¼–ç¨‹æ¨¡å—
import asyncio
# å¯¼å…¥æ£€æŸ¥æ¨¡å—ï¼Œç”¨äºæ£€æŸ¥å¯¹è±¡ç±»å‹
import inspect
# å¯¼å…¥æ—¥å¿—è®°å½•æ¨¡å—
import logging
# å¯¼å…¥æ—¥å¿—é…ç½®æ¨¡å—
import logging.config
# å¯¼å…¥LightRAGæ ¸å¿ƒç»„ä»¶ï¼šä¸»ç±»å’ŒæŸ¥è¯¢å‚æ•°ç±»
from lightrag import LightRAG, QueryParam
# å¯¼å…¥OpenAIå…¼å®¹çš„å®Œæˆå‡½æ•°
from lightrag.llm.openai import openai_complete_if_cache
# å¯¼å…¥å·¥å…·å‡½æ•°ï¼šåµŒå…¥å‡½æ•°ç±»ã€æ—¥å¿—å™¨ã€è¯¦ç»†è°ƒè¯•è®¾ç½®
from lightrag.utils import EmbeddingFunc, logger, set_verbose_debug
# å¯¼å…¥å…±äº«å­˜å‚¨çš„ç®¡é“çŠ¶æ€åˆå§‹åŒ–å‡½æ•°
from lightrag.kg.shared_storage import initialize_pipeline_status
# å¯¼å…¥PyTorchæ·±åº¦å­¦ä¹ æ¡†æ¶
import torch
import numpy as np
import time

# å¯¼å…¥æˆ‘ä»¬çš„ LightRAG Qwen é€‚é…å™¨ï¼ˆç”¨äºembeddingï¼‰
from lightrag_qwen_adapter import get_lightrag_embedding_func

# å¯¼å…¥ Qwen LLM é€‚é…å™¨ï¼ˆç”¨äº LLMï¼Œé€šè¿‡ HTTP APIï¼‰
from qwen_llm_adapter import qwen_llm_model_func, MODEL_CONFIG

# å¯¼å…¥ç®€åŒ–çš„å…±äº«ç®¡ç†å™¨ç”¨äºçŠ¶æ€æ£€æŸ¥
from shared_qwen_manager import simple_qwen

# ä¿®æ”¹æ¨¡å‹è·¯å¾„
MODEL_CONFIG["llm_model_path"] = "/data/qwen4btune_w4bf16_seq8192_bm1684x_1dev_20250721_195513.bmodel"
MODEL_CONFIG["config_path"] = "/data/LLM-TPU/models/Qwen3/python_demo/config"
MODEL_CONFIG["temperature"] = 0.5

# å®šä¹‰å·¥ä½œç›®å½•è·¯å¾„
WORKING_DIR = "./result/biography_final"
# åœ¨ WORKING_DIR å®šä¹‰åæ·»åŠ è¾“å‡ºç›®å½•é…ç½®
OUTPUT_DIR = "/data/mermaidRender/dist"

# ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
if not os.path.exists(OUTPUT_DIR):
    print(f"Creating output directory: {OUTPUT_DIR}")
    os.makedirs(OUTPUT_DIR)
else:
    print(f"Output directory already exists: {OUTPUT_DIR},continue...")
    
# å¦‚æœå·¥ä½œç›®å½•ä¸å­˜åœ¨ï¼Œåˆ™åˆ›å»ºè¯¥ç›®å½•
if not os.path.exists(WORKING_DIR):
    print(f"Setting directory not exist,creating working directory: {WORKING_DIR}")
    os.mkdir(WORKING_DIR)
else:
    print(f"Working directory already exists: {WORKING_DIR},continue...")

# æ–‡ä»¶ç›‘æ§å¤„ç†å™¨ç±» - ä¿®æ”¹ç‰ˆæœ¬
class QuestionFileHandler(FileSystemEventHandler):
    """é—®é¢˜æ–‡ä»¶å˜åŒ–ç›‘æ§å¤„ç†å™¨"""
    def __init__(self, callback, loop):
        self.callback = callback
        self.last_content_hash = None
        self.loop = loop  # ä¿å­˜äº‹ä»¶å¾ªç¯å¼•ç”¨
        
    def on_modified(self, event):
        if not event.is_directory and event.src_path.endswith('ragQuestions.txt'):
            # æ£€æŸ¥æ–‡ä»¶å†…å®¹æ˜¯å¦çœŸçš„æ”¹å˜äº†
            try:
                with open(event.src_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                content_hash = hashlib.md5(content.encode()).hexdigest()
                
                if content_hash != self.last_content_hash:
                    self.last_content_hash = content_hash
                    # ä½¿ç”¨ run_coroutine_threadsafe å®‰å…¨åœ°åœ¨äº‹ä»¶å¾ªç¯ä¸­è°ƒåº¦åç¨‹
                    asyncio.run_coroutine_threadsafe(
                        self.callback(event.src_path), 
                        self.loop
                    )
            except Exception as e:
                print(f"è¯»å–æ–‡ä»¶æ—¶å‡ºé”™: {e}")

# å®šä¹‰æ—¥å¿—é…ç½®å‡½æ•°
def configure_logging():
    """é…ç½®åº”ç”¨ç¨‹åºçš„æ—¥å¿—è®°å½•"""
    # é‡ç½®ç°æœ‰çš„å¤„ç†ç¨‹åºä»¥ç¡®ä¿å¹²å‡€çš„é…ç½®
    for logger_name in ["uvicorn", "uvicorn.access", "uvicorn.error", "lightrag"]:
        logger_instance = logging.getLogger(logger_name)
        logger_instance.handlers = []
        logger_instance.filters = []

    # ä»ç¯å¢ƒå˜é‡è·å–æ—¥å¿—ç›®å½•è·¯å¾„ï¼Œé»˜è®¤ä½¿ç”¨å½“å‰ç›®å½•
    log_dir = os.getenv("LOG_DIR", os.getcwd())
    # æ„å»ºæ—¥å¿—æ–‡ä»¶çš„ç»å¯¹è·¯å¾„
    log_file_path = os.path.abspath(
        os.path.join(log_dir, "lightrag_qwen_bmodel_demo.log")
    )

    print(f"\nLightRAG Qwen embedding bmodel demo log file: {log_file_path}\n")
    os.makedirs(os.path.dirname(log_dir), exist_ok=True)

    # ä»ç¯å¢ƒå˜é‡è·å–æ—¥å¿—æ–‡ä»¶æœ€å¤§å¤§å°å’Œå¤‡ä»½æ•°é‡
    log_max_bytes = int(os.getenv("LOG_MAX_BYTES", 10485760))  # é»˜è®¤10MB
    log_backup_count = int(os.getenv("LOG_BACKUP_COUNT", 5))  # é»˜è®¤5ä¸ªå¤‡ä»½

    # é…ç½®æ—¥å¿—ç³»ç»Ÿ
    logging.config.dictConfig(
        {
            "version": 1,
            "disable_existing_loggers": False,
            "formatters": {
                "default": {
                    "format": "%(levelname)s: %(message)s",
                },
                "detailed": {
                    "format": "%(asctime)s - %(name)s - %(levelname)s - %(message)s",
                },
            },
            "handlers": {
                "console": {
                    "formatter": "default",
                    "class": "logging.StreamHandler",
                    "stream": "ext://sys.stderr",
                },
                "file": {
                    "formatter": "detailed",
                    "class": "logging.handlers.RotatingFileHandler",
                    "filename": log_file_path,
                    "maxBytes": log_max_bytes,
                    "backupCount": log_backup_count,
                    "encoding": "utf-8",
                },
            },
            "loggers": {
                "lightrag": {
                    "handlers": ["console", "file"],
                    "level": "INFO",
                    "propagate": False,
                },
            },
        }
    )

    logger.setLevel(logging.INFO)
    set_verbose_debug(os.getenv("VERBOSE_DEBUG", "false").lower() == "true")

# å®šä¹‰æµå¼è¾“å‡ºæ‰“å°å‡½æ•°
async def print_stream(stream):
    async for chunk in stream:
        if chunk:
            print(chunk, end="", flush=True)

# æ£€æŸ¥æ¨¡å‹çŠ¶æ€çš„å‡½æ•°
def check_model_availability():
    """æ£€æŸ¥æ¨¡å‹æ˜¯å¦å¯ç”¨"""
    try:
        status = simple_qwen.get_status()
        print(f"æ¨¡å‹çŠ¶æ€æ£€æŸ¥: {status}")
        
        if status["local_model"] or status["external_model"]:
            print("âœ… æ£€æµ‹åˆ°å¯ç”¨çš„ LLM æ¨¡å‹")
            return True
        else:
            print("âš ï¸ æœªæ£€æµ‹åˆ°å¯ç”¨çš„ LLM æ¨¡å‹")
            return False
    except Exception as e:
        print(f"âŒ æ¨¡å‹çŠ¶æ€æ£€æŸ¥å¤±è´¥: {e}")
        return False

# æ£€æŸ¥ API æœåŠ¡å™¨çŠ¶æ€
def check_api_server_status():
    """æ£€æŸ¥APIæœåŠ¡å™¨çŠ¶æ€"""
    try:
        import requests
        response = requests.get("http://localhost:8899/status", timeout=5)
        if response.status_code == 200:
            status = response.json()
            print(f"âœ… APIæœåŠ¡å™¨å¯ç”¨: {status}")
            return True
        else:
            print(f"âš ï¸ APIæœåŠ¡å™¨å“åº”å¼‚å¸¸: {response.status_code}")
            return False
    except Exception as e:
        print(f"âŒ APIæœåŠ¡å™¨ä¸å¯ç”¨: {e}")
        return False

async def initialize_rag():
    """ä½¿ç”¨ Qwen embedding bmodel + HTTP LLM API åˆå§‹åŒ– RAG ç³»ç»Ÿ"""
    
    print("æ­£åœ¨åˆå§‹åŒ– LightRAG with Qwen embedding bmodel + HTTP LLM API...")
    
    # 1. æ£€æŸ¥ LLM æ¨¡å‹çŠ¶æ€
    print("\n=== æ£€æŸ¥ LLM æ¨¡å‹çŠ¶æ€ ===")
    llm_available = check_model_availability()
    api_available = check_api_server_status()
    
    if not (llm_available or api_available):
        print("âš ï¸ è­¦å‘Šï¼šLLM æ¨¡å‹å’ŒAPIæœåŠ¡å™¨éƒ½ä¸å¯ç”¨")
        print("è¯·ç¡®ä¿ sample_audio.py æ­£åœ¨è¿è¡Œ")
        print("ç»§ç»­åˆå§‹åŒ–ï¼Œä½†LLMåŠŸèƒ½å¯èƒ½å—é™...")
    else:
        print("âœ… LLM æ¨¡å‹é€šè¿‡ HTTP API å¯ç”¨")
    
    # 2. åˆå§‹åŒ– Embedding æ¨¡å‹ï¼ˆæœ¬åœ°åŠ è½½ï¼‰
    print("\n=== åˆå§‹åŒ– Embedding æ¨¡å‹ ===")
    try:
        qwen_embedding_func = get_lightrag_embedding_func(
            model_path='/data/Qwen3_Embedding_0.6B_my_1684x_128_f16.bmodel',
            tokenizer_path=None,
            batch_size=1,
            device="tpu",
            async_mode=True
        )
        print("âœ… Embedding æ¨¡å‹åˆå§‹åŒ–æˆåŠŸï¼ˆæœ¬åœ°bmodelï¼‰")
    except Exception as e:
        print(f"âŒ Embedding æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
        raise
    
    # 3. åˆ›å»º LightRAG å®ä¾‹
    print("\n=== åˆ›å»º LightRAG å®ä¾‹ ===")
    try:
        # åˆ›å»ºè‡ªå®šä¹‰ tokenizer æ¥æ›¿ä»£ tiktoken
        class SimpleChineseTokenizer:
            """ç®€å•çš„ä¸­æ–‡åˆ†è¯å™¨ï¼Œé¿å…ä½¿ç”¨ tiktoken"""
            
            def encode(self, text: str) -> list[int]:
                """å°†æ–‡æœ¬ç¼–ç ä¸º token ID åˆ—è¡¨"""
                # å­—ç¬¦çº§ç¼–ç ï¼Œé€‚åˆä¸­æ–‡
                return [ord(c) for c in text if ord(c) <= 65535]  # è¿‡æ»¤è¶…å‡ºBMPçš„å­—ç¬¦
            
            def decode(self, tokens: list[int]) -> str:
                """å°† token ID åˆ—è¡¨è§£ç ä¸ºæ–‡æœ¬"""
                try:
                    return ''.join([chr(t) for t in tokens if 0 <= t <= 65535])
                except ValueError:
                    return ""
        
        # åˆ›å»º tokenizer å®ä¾‹
        from lightrag.utils import Tokenizer
        custom_tokenizer = Tokenizer(
            model_name="chinese_custom", 
            tokenizer=SimpleChineseTokenizer()
        )
        
        print("âœ… è‡ªå®šä¹‰ tokenizer åˆ›å»ºæˆåŠŸï¼ˆæ— éœ€ tiktokenï¼‰")
        
        # åˆ›å»ºLightRAGå®ä¾‹ï¼Œä½¿ç”¨HTTP APIçš„LLM + æœ¬åœ°embedding + è‡ªå®šä¹‰tokenizer
        rag = LightRAG(
            working_dir=WORKING_DIR,
            llm_model_func=qwen_llm_model_func,  # ä½¿ç”¨HTTP API
            embedding_func=EmbeddingFunc(
                embedding_dim=1024,
                max_token_size=5000,
                func=qwen_embedding_func,  # ä½¿ç”¨æœ¬åœ°bmodel
            ),
            tokenizer=custom_tokenizer,  # å…³é”®ï¼šä½¿ç”¨è‡ªå®šä¹‰tokenizer
            tiktoken_model_name=None,     # ç¡®ä¿ä¸ä½¿ç”¨tiktoken
        )
        print("âœ… LightRAG å®ä¾‹åˆ›å»ºæˆåŠŸ")
        
    except Exception as e:
        print(f"âŒ LightRAG åˆå§‹åŒ–å¤±è´¥: {e}")
        print("å°è¯•ä½¿ç”¨æœ€å°é…ç½®é‡æ–°åˆå§‹åŒ–...")
        
        # å¤‡ç”¨æ–¹æ¡ˆï¼šæ›´ç®€å•çš„tokenizer
        class BasicTokenizer:
            def encode(self, text: str) -> list[int]:
                # æœ€ç®€å•çš„å­—èŠ‚ç¼–ç 
                return list(text.encode('utf-8'))
            
            def decode(self, tokens: list[int]) -> str:
                try:
                    return bytes(tokens).decode('utf-8', errors='ignore')
                except:
                    return ""
        
        from lightrag.utils import Tokenizer
        basic_tokenizer = Tokenizer(
            model_name="basic", 
            tokenizer=BasicTokenizer()
        )
        
        rag = LightRAG(
            working_dir=WORKING_DIR,
            llm_model_func=qwen_llm_model_func,
            embedding_func=EmbeddingFunc(
                embedding_dim=1024,
                max_token_size=5000,
                func=qwen_embedding_func,
            ),
            tokenizer=basic_tokenizer,    # ä½¿ç”¨åŸºç¡€tokenizer
            tiktoken_model_name=None,
        )

    # 4. åˆå§‹åŒ–å­˜å‚¨ç³»ç»Ÿ
    print("\n=== åˆå§‹åŒ–å­˜å‚¨ç³»ç»Ÿ ===")
    try:
        await rag.initialize_storages()
        await initialize_pipeline_status()
        print("âœ… å­˜å‚¨ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    except Exception as e:
        print(f"âš ï¸ å­˜å‚¨ç³»ç»Ÿåˆå§‹åŒ–å‡ºç°é—®é¢˜: {e}")
        print("ç»§ç»­è¿è¡Œ...")

    print("âœ… LightRAG åˆå§‹åŒ–å®Œæˆï¼ˆEmbeddingæœ¬åœ° + LLMè¿œç¨‹APIï¼‰")
    return rag

# é—®é¢˜æ–‡ä»¶è§£æå‡½æ•°
def parse_question_file(file_path: str) -> tuple:
    """è§£æé—®é¢˜æ–‡ä»¶ï¼Œè¿”å› (åºå·, é—®é¢˜å†…å®¹)"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            lines = [line.strip() for line in f.readlines() if line.strip()]
        
        if len(lines) >= 2:
            sequence_num = lines[0]
            question = lines[1]
            return sequence_num, question
        elif len(lines) == 1:
            # åªæœ‰ä¸€è¡Œï¼Œå½“ä½œé—®é¢˜å¤„ç†
            return "1", lines[0]
        else:
            return None, None
            
    except FileNotFoundError:
        print(f"é—®é¢˜æ–‡ä»¶ {file_path} ä¸å­˜åœ¨")
        return None, None
    except Exception as e:
        print(f"è§£æé—®é¢˜æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return None, None

# æ–‡æ¡£æ¥å£å‡½æ•°
def read_questions_from_file(file_path: str) -> list:
    """ä»æ–‡ä»¶ä¸­è¯»å–é—®é¢˜åˆ—è¡¨"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            questions = [line.strip() for line in f if line.strip()]
        return questions
    except FileNotFoundError:
        print(f"Warning: {file_path} not found, using default question")
        return ["ä»‹ç»ç™½è¡€ç—…ä¸ç™½ç»†èƒçš„å…³ç³»"]  # é»˜è®¤é—®é¢˜

def save_chunk_ids(chunk_ids: list, file_path: str):
    """å°†chunk IDä¿å­˜åˆ°æ–‡ä»¶"""
    with open(file_path, 'w', encoding='utf-8') as f:
        for chunk_id in chunk_ids:
            f.write(f"{chunk_id}\n")
    print(f"Chunk IDs saved to: {file_path}")

# å¤„ç†å•ä¸ªé—®é¢˜çš„å‡½æ•°
async def process_single_question(rag, question: str, sequence_num: str):
    """å¤„ç†å•ä¸ªé—®é¢˜çš„å®Œæ•´æµç¨‹"""
    print(f"\n{'='*60}")
    print(f"å¤„ç†é—®é¢˜åºå· {sequence_num}: {question}")
    print('='*60)
    
    try:
        # 1. ç›´æ¥è°ƒç”¨ chunks_vdb.query æ¥è·å–æ£€ç´¢ç»“æœ
        print("\n=====================")
        print("Direct chunk search")
        print("=====================")
        start_time = time.time()
        
        try:
            chunks_result = await rag.chunks_vdb.query(
                query=question,
                top_k=5,
                ids=None
            )
            
            # æå– chunk IDs
            chunk_ids = [chunk.get('id', 'N/A') for chunk in chunks_result]
            
            # æ‰“å°æŸ¥è¯¢ç»“æœ
            print(f"æ£€ç´¢åˆ° {len(chunks_result)} ä¸ªç›¸å…³ chunks:")
            for i, chunk in enumerate(chunks_result):
                print(f"\nChunk {i+1}:")
                print(f"  ID: {chunk.get('id', 'N/A')}")
                print(f"  Score: {chunk.get('distance', 'N/A')}")
                print(f"  Content: {chunk.get('content', '')[:100]}...")
                print(f"  Full Doc ID: {chunk.get('full_doc_id', 'N/A')}")
                print(f"  Chunk Order Index: {chunk.get('chunk_order_index', 'N/A')}")
                print(f"  File Path: {chunk.get('file_path', 'N/A')}")

            # 2. ä¿å­˜ chunk IDs åˆ° ragSearch.txt
            chunk_ids_file = os.path.join(OUTPUT_DIR, "ragSearch.txt")
            save_chunk_ids(chunk_ids, chunk_ids_file)
            
        except Exception as e:
            print(f"âš ï¸ Chunk æ£€ç´¢å¤±è´¥: {e}")
            chunk_ids_file = os.path.join(OUTPUT_DIR, "ragSearch.txt")
            save_chunk_ids(["æ£€ç´¢å¤±è´¥"], chunk_ids_file)
        
        # 3. æ‰§è¡Œå®Œæ•´çš„RAGæŸ¥è¯¢
        print(f"\n=====================")
        print("Query mode: naive - Full RAG Search")
        print("=====================")
        start_time = time.time()
        
        try:
            resp = await rag.aquery(
                question,
                param=QueryParam(mode="naive", stream=True),
            )
            end_time = time.time()
            print(f"Search execution time: {end_time - start_time:.2f} seconds")
            
            # æ›¿æ¢åŸæœ‰çš„ if inspect.isasyncgen(resp): ... else: ... é€»è¾‘
            response_file = os.path.join(OUTPUT_DIR, "RAGResult.txt")

            # ç¡®ä¿æ–‡ä»¶åœ¨ä½¿ç”¨å‰æ˜¯ç©ºçš„
            with open(response_file, 'w', encoding='utf-8') as f:
                f.write("") # æ¸…ç©ºæ–‡ä»¶

            print("Response lightrag_bmodel2 success:")
            # ä»¥è¿½åŠ æ¨¡å¼æ‰“å¼€æ–‡ä»¶ï¼Œåœ¨å¾ªç¯ä¸­å†™å…¥
            with open(response_file, 'a', encoding='utf-8') as f:
                if inspect.isasyncgen(resp):
                    # æµå¼å“åº”
                    async for chunk in resp:
                        if chunk:
                            # åŒæ—¶è¾“å‡ºåˆ°ç»ˆç«¯å’Œæ–‡ä»¶
                            print(chunk, end="", flush=True)
                            f.write(chunk)
                            f.flush() # å…³é”®ï¼šç¡®ä¿ç«‹å³å†™å…¥ç£ç›˜
                else:
                    # éæµå¼å“åº”ï¼ˆä½œä¸ºå¤‡ç”¨é€»è¾‘ï¼‰
                    response_text = str(resp)
                    print(response_text)
                    f.write(response_text)

            # å¾ªç¯ç»“æŸåæ¢è¡Œï¼Œç¾åŒ–ç»ˆç«¯è¾“å‡º
            print() 

            print(f"\né—®é¢˜åºå· {sequence_num} å¤„ç†å®Œæˆ")
            print(f"- Chunk IDs ä¿å­˜è‡³: {chunk_ids_file}")
            print(f"- å“åº”æµå¼ä¿å­˜è‡³: {response_file}")

            return True
            
        except Exception as e:
            print(f"âš ï¸ RAG æŸ¥è¯¢å¤±è´¥: {e}")
            # å°è¯•ç›´æ¥ä½¿ç”¨ LLM
            print("å°è¯•ç›´æ¥ä½¿ç”¨ LLM å›ç­”...")
            
            return False
        
    except Exception as e:
        print(f"å¤„ç†é—®é¢˜æ—¶å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

        return False

# ç›‘æ§å’Œå¤„ç†é—®é¢˜çš„ä¸»å‡½æ•° - ä¿®æ”¹ç‰ˆæœ¬
async def monitor_and_process_questions(rag):
    """ç›‘æ§é—®é¢˜æ–‡ä»¶å¹¶å¤„ç†"""
    questions_file = os.path.join(OUTPUT_DIR, "ragQuestions.txt")
    processed_sequences = set()  # è®°å½•å·²å¤„ç†çš„åºå·
    
    async def handle_file_change(file_path):
        """å¤„ç†æ–‡ä»¶å˜åŒ–"""
        try:
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ä¸ºç©º
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read().strip()
            
            if not content:
                print("é—®é¢˜æ–‡ä»¶ä¸ºç©ºï¼Œç­‰å¾…ç”¨æˆ·è¾“å…¥é—®é¢˜...")
                return
            
            sequence_num, question = parse_question_file(file_path)
            
            if sequence_num and question:
                # æ£€æŸ¥æ˜¯å¦æ˜¯æ–°é—®é¢˜ï¼ˆåŸºäºåºå·ï¼‰
                if sequence_num not in processed_sequences:
                    print(f"\næ£€æµ‹åˆ°æ–°é—®é¢˜: åºå· {sequence_num}")
                    success = await process_single_question(rag, question, sequence_num)
                    
                    if success:
                        processed_sequences.add(sequence_num)
                        print(f"é—®é¢˜åºå· {sequence_num} å¤„ç†æˆåŠŸ")
                    else:
                        print(f"é—®é¢˜åºå· {sequence_num} å¤„ç†å¤±è´¥")
                else:
                    print(f"é—®é¢˜åºå· {sequence_num} å·²å¤„ç†è¿‡ï¼Œè·³è¿‡")
            else:
                print("é—®é¢˜æ–‡ä»¶æ ¼å¼ä¸æ­£ç¡®æˆ–ä¸ºç©º")
        except Exception as e:
            print(f"å¤„ç†æ–‡ä»¶å˜åŒ–æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
    
    # è·å–å½“å‰äº‹ä»¶å¾ªç¯
    loop = asyncio.get_running_loop()
    
    # åˆ›å»ºæ–‡ä»¶ç›‘æ§å¤„ç†å™¨ï¼Œä¼ å…¥äº‹ä»¶å¾ªç¯
    event_handler = QuestionFileHandler(callback=handle_file_change, loop=loop)
    
    # è®¾ç½®æ–‡ä»¶ç›‘æ§
    observer = Observer()
    observer.schedule(event_handler, OUTPUT_DIR, recursive=False)
    observer.start()
    
    print(f"å¼€å§‹ç›‘æ§é—®é¢˜æ–‡ä»¶: {questions_file}")
    print("è¯·åœ¨ ragQuestions.txt ä¸­å†™å…¥é—®é¢˜ï¼Œæ ¼å¼å¦‚ä¸‹:")
    print("ç¬¬ä¸€è¡Œ: åºå·")
    print("ç¬¬äºŒè¡Œ: é—®é¢˜å†…å®¹")
    print("æŒ‰ Ctrl+C é€€å‡ºç›‘æ§")
    
    # åˆå§‹åŒ–æ—¶è¯»å–å½“å‰åºå·ï¼Œä½†ä¸å¤„ç†
    if os.path.exists(questions_file):
        try:
            sequence_num, _ = parse_question_file(questions_file)
            if sequence_num:
                processed_sequences.add(sequence_num)
                print(f"æ£€æµ‹åˆ°ç°æœ‰é—®é¢˜æ–‡ä»¶ï¼Œå½“å‰åºå·: {sequence_num}ï¼Œç­‰å¾…åºå·å˜åŒ–...")
        except:
            pass
    
    try:
        # ä¿æŒç›‘æ§è¿è¡Œ
        while True:
            await asyncio.sleep(1)
    except KeyboardInterrupt:
        print("\nåœæ­¢æ–‡ä»¶ç›‘æ§")
        observer.stop()
    
    observer.join()

def create_example_question_file():
    """åˆ›å»ºç©ºçš„é—®é¢˜æ–‡ä»¶"""
    questions_file = os.path.join(OUTPUT_DIR, "ragQuestions.txt")
    if not os.path.exists(questions_file):
        # åˆ›å»ºç©ºæ–‡ä»¶ï¼Œä¸å†™å…¥ä»»ä½•å†…å®¹
        with open(questions_file, 'w', encoding='utf-8') as f:
            f.write("")
        
        print(f"åˆ›å»ºç©ºçš„é—®é¢˜æ–‡ä»¶: {questions_file}")
        print("ç­‰å¾…ç”¨æˆ·è¾“å…¥é—®é¢˜...")
    else:
        print(f"é—®é¢˜æ–‡ä»¶å·²å­˜åœ¨: {questions_file}")
        print("ç­‰å¾…é—®é¢˜åºå·å˜åŒ–...")

# å®šä¹‰å¼‚æ­¥ä¸»å‡½æ•°
async def main():
    try:
        # é¦–å…ˆæ£€æŸ¥æ¨¡å‹çŠ¶æ€
        print("=" * 60)
        print("æ£€æŸ¥ç³»ç»ŸçŠ¶æ€...")
        print("=" * 60)
        
        llm_available = check_model_availability()
        api_available = check_api_server_status()
        
        if not (llm_available or api_available):
            print("âš ï¸ è­¦å‘Šï¼šLLMæ¨¡å‹å¯èƒ½æœªå‡†å¤‡å°±ç»ª")
            print("è¯·ç¡®ä¿ sample_audio.py æ­£åœ¨è¿è¡Œ")
            print("ç»§ç»­åˆå§‹åŒ–ï¼Œå°†å°è¯•é€šè¿‡APIè®¿é—®...")
        
        # åˆå§‹åŒ–RAGå®ä¾‹
        print("\n" + "=" * 60)
        print("åˆå§‹åŒ– RAG ç³»ç»Ÿ...")
        print("=" * 60)
        
        rag = await initialize_rag()

        # # æµ‹è¯•åµŒå…¥å‡½æ•°
        # test_text = ["This is a test string for embedding with Qwen embedding bmodel."]
        # print("\n=======================")
        # print("Testing Qwen embedding bmodel embedding function")
        # print("========================")
        # print(f"Test text: {test_text}")
        
        # try:
        #     embedding = await rag.embedding_func(test_text)
        #     embedding_dim = embedding.shape[1]
        #     print(f"âœ… Detected embedding dimension: {embedding_dim}")
        #     print(f"Embedding shape: {embedding.shape}")
        #     print(f"First 10 values: {embedding[0][:10]}\n")
        # except Exception as e:
        #     print(f"âš ï¸ Embedding æµ‹è¯•å¤±è´¥: {e}")

        # # æµ‹è¯•LLMå‡½æ•°
        # print("\n=======================")
        # print("Testing Qwen LLM HTTP API function")
        # print("========================")
        # test_prompt = "ä½ å¥½ï¼Œè¯·ç®€å•ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚"
        # print(f"Test prompt: {test_prompt}")
        
        # try:
        #     llm_response = await rag.llm_model_func(
        #         test_prompt,
        #         system_prompt="ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹ã€‚"
        #     )
        #     print(f"âœ… LLM Response: {llm_response}\n")
        # except Exception as e:
        #     print(f"âš ï¸ LLM æµ‹è¯•å¤±è´¥: {e}")
        #     print("å¯èƒ½åŸå› ï¼šsample_audio.py æœªè¿è¡Œæˆ–APIæœåŠ¡å™¨æœªå¯åŠ¨")
        
        # åˆ›å»ºç¤ºä¾‹é—®é¢˜æ–‡ä»¶
        create_example_question_file()
        
        # å¼€å§‹ç›‘æ§å’Œå¤„ç†é—®é¢˜
        print("\n" + "=" * 60)
        print("å¼€å§‹é—®é¢˜ç›‘æ§...")
        print("=" * 60)
        
        await monitor_and_process_questions(rag)
        
    except Exception as e:
        print(f"An error occurred: {e}")
        import traceback
        traceback.print_exc()
    finally:
        if 'rag' in locals():
            try:
                await rag.finalize_storages()
            except:
                pass

# ç¨‹åºå…¥å£ç‚¹
if __name__ == "__main__":
    # åœ¨è¿è¡Œä¸»å‡½æ•°å‰é…ç½®æ—¥å¿—
    configure_logging()
    
    print("ğŸš€ å¯åŠ¨ LightRAG with Qwen bmodel (æ··åˆæ¨¡å¼)")
    print("=" * 60)
    print("æ¶æ„è¯´æ˜:")
    print("- LLM: é€šè¿‡ HTTP API è®¿é—® (qwen_llm_adapter.py)")
    print("- Embedding: ç›´æ¥åŠ è½½æœ¬åœ° bmodel")
    print("- æ³¨æ„ï¼šè¯·ç¡®ä¿ sample_audio.py æ­£åœ¨è¿è¡Œä»¥æä¾›LLMæœåŠ¡")
    print("=" * 60)
    
    # è¿è¡Œå¼‚æ­¥ä¸»å‡½æ•°
    asyncio.run(main())
    
    # æ‰“å°å®Œæˆä¿¡æ¯
    print("\nDone! Qwen embedding bmodel integration with LightRAG completed successfully.")
'''
'''sample_audio.py
import pyaudio
import wave
import threading
import queue
import os
import time
import json
from datetime import datetime
import globalConfig
import random
from http.server import HTTPServer, BaseHTTPRequestHandler
import json
import urllib.parse

os.environ['TOKENIZERS_PARALLELISM'] = 'false'

# å¯¼å…¥ç®€åŒ–çš„å…±äº«ç®¡ç†å™¨
from shared_qwen_manager import simple_qwen, generate_mermaid, generate_text

# å‚æ•°è®¾ç½®
FORMAT = pyaudio.paInt16  # 16ä½é‡‡æ ·
CHANNELS = 1              # å•å£°é“
RATE = 16000              # é‡‡æ ·ç‡ï¼ˆHzï¼‰
CHUNK = 1024              # æ¯ä¸ªç¼“å†²åŒºçš„å¸§æ•°
RECORD_SECONDS = 30       # æ¯ä¸ªæ–‡ä»¶å½•éŸ³æ—¶é•¿
RECORDING_FOLDER = "recordings"  # å½•éŸ³æ–‡ä»¶ä¿å­˜æ–‡ä»¶å¤¹

# åˆ›å»ºä¿å­˜å½•éŸ³çš„æ–‡ä»¶å¤¹
os.makedirs(RECORDING_FOLDER, exist_ok=True)

# åˆ›å»ºä¸€ä¸ªé˜Ÿåˆ—ç”¨äºå­˜å‚¨å¾…å¤„ç†çš„éŸ³é¢‘æ–‡ä»¶
audio_queue = queue.Queue()

exitFlag = False

class QwenAPIHandler(BaseHTTPRequestHandler):
    """ç®€å•çš„HTTP APIå¤„ç†å™¨ï¼Œç”¨äºè·¨è¿›ç¨‹æ¨ç†"""
    
    def do_POST(self):
        if self.path == '/generate':
            try:
                # è¯»å–è¯·æ±‚æ•°æ®
                content_length = int(self.headers['Content-Length'])
                post_data = self.rfile.read(content_length)
                data = json.loads(post_data.decode('utf-8'))
                
                prompt = data.get('prompt', '')
                max_new_tokens = data.get('max_new_tokens', 512)
                
                if not prompt:
                    self.send_error(400, "Missing prompt")
                    return
            
                self.send_response(200)
                self.send_header('Content-type', 'text/plain; charset=utf-8')
                # å…³é”®ï¼šå¯ç”¨åˆ†å—ä¼ è¾“
                self.send_header('Transfer-Encoding', 'chunked') 
                self.end_headers()

                # result_generator æ˜¯ä¸€ä¸ªç”Ÿæˆå™¨
                result_generator = generate_text(prompt, max_new_tokens)

                for chunk in result_generator:
                    if chunk:
                        # ç¼–ç æ•°æ®å—
                        encoded_chunk = chunk.encode('utf-8')
                        # è·å–æ•°æ®å—é•¿åº¦çš„åå…­è¿›åˆ¶è¡¨ç¤º
                        chunk_size = f"{len(encoded_chunk):X}\r\n".encode('utf-8')
                        # å†™å…¥å—å¤§å°
                        self.wfile.write(chunk_size)
                        # å†™å…¥æ•°æ®å—
                        self.wfile.write(encoded_chunk)
                        # å†™å…¥å—ç»“æŸç¬¦
                        self.wfile.write(b"\r\n")
                        # åˆ·æ–°ç¼“å†²åŒºï¼Œç¡®ä¿ç«‹å³å‘é€
                        self.wfile.flush()

                # å‘é€ä¸€ä¸ªå¤§å°ä¸º0çš„å—ï¼Œè¡¨ç¤ºä¼ è¾“ç»“æŸ
                self.wfile.write(b"0\r\n\r\n")
                self.wfile.flush()
                
            except Exception as e:
                error_response = {'error': str(e), 'status': 'error'}
                self.send_response(500)
                self.send_header('Content-type', 'application/json')
                self.end_headers()
                self.wfile.write(json.dumps(error_response).encode('utf-8'))
    
    def do_GET(self):
        if self.path == '/status':
            # è¿”å›æ¨¡å‹çŠ¶æ€
            status = simple_qwen.get_status()
            self.send_response(200)
            self.send_header('Content-type', 'application/json')
            self.end_headers()
            self.wfile.write(json.dumps(status).encode('utf-8'))
        else:
            self.send_error(404, "Not found")
    
    def log_message(self, format, *args):
        # ç¦ç”¨é»˜è®¤æ—¥å¿—è¾“å‡º
        pass

def start_api_server():
    """å¯åŠ¨APIæœåŠ¡å™¨"""
    try:
        server = HTTPServer(('localhost', 8899), QwenAPIHandler)
        print("ğŸŒ APIæœåŠ¡å™¨å¯åŠ¨åœ¨ http://localhost:8899")
        print("  - POST /generate : æ–‡æœ¬ç”Ÿæˆ")
        print("  - GET /status : æ¨¡å‹çŠ¶æ€")
        server.serve_forever()
    except Exception as e:
        print(f"âŒ APIæœåŠ¡å™¨å¯åŠ¨å¤±è´¥: {e}")

def initEnv():
    import os
    # è®¾ç½®ç¯å¢ƒå˜é‡
    os.environ["LOG_LEVEL"] = "-1"
    os.environ["LD_LIBRARY_PATH"] = "/opt/sophon/libsophon-current/lib:" + os.environ.get("LD_LIBRARY_PATH", "")

    # å¯åŠ¨ pulseaudioï¼ˆå¿½ç•¥é”™è¯¯ï¼‰
    os.system("pulseaudio --start 2>/dev/null || true")

def initMermaid(text):
    template = f"""
graph LR
    A[{text}å¼€å¯å…¨æ–°æ•™å­¦æ¨¡å¼] --> B[æ€ç»´å¯¼å›¾]
    A --> C[è¯¾æœ¬æŸ¥è¯¢]
    A --> D[äº’åŠ¨æé—®]
    A --> E[å†å²ç¬”è®°]
"""
    # Ensure the output directory exists
    os.makedirs('/data/mermaidRender/text', exist_ok=True)
    with open('/data/mermaidRender/text/outmermaid.mmd', "w") as f:
        f.write(template)

def record_audio():
    """å½•éŸ³çº¿ç¨‹ï¼šæŒç»­å½•éŸ³å¹¶æ¯10ç§’ä¿å­˜ä¸€ä¸ªæ–‡ä»¶"""
    # åˆå§‹åŒ–PyAudio
    audio = pyaudio.PyAudio()
    
    # æ‰“å¼€æµ
    stream = audio.open(format=FORMAT,
                        channels=CHANNELS,
                        rate=RATE,
                        input=True,
                        frames_per_buffer=CHUNK)
    
    print("å½•éŸ³çº¿ç¨‹å·²å¯åŠ¨")
    last_state = True
    try:
        while True:
            if exitFlag:
                print("record_audioçº¿ç¨‹é€€å‡º")
                break
            if not globalConfig.running:
                time.sleep(1)
                last_state = False
                continue
            # æ¸…ç©ºè¾“å…¥ç¼“å†²åŒºï¼Œé¿å…æ®‹ç•™æ•°æ®å½±å“æ–°å½•éŸ³
            if(not last_state):
                print("å½•éŸ³çº¿ç¨‹å¼€å§‹ï¼Œæ¸…ç©ºè¾“å…¥ç¼“å†²åŒº...")
                initMermaid("æ­£åœ¨é‡‡é›†ä¸­...")
                last_state = True
                stream.stop_stream()
                stream.start_stream()

            frames = []
            filename = os.path.join(
                RECORDING_FOLDER,
                f"audio_{datetime.now().strftime('%Y%m%d_%H%M%S_%f')}_{random.randint(1000,9999)}.wav"
            )
            print(f"å¼€å§‹å½•åˆ¶æ–‡ä»¶: {filename}")
            
            # å½•åˆ¶æŒ‡å®šç§’æ•°çš„éŸ³é¢‘
            for _ in range(0, int(RATE / CHUNK * RECORD_SECONDS)):
                if not globalConfig.running:
                    break
                data = stream.read(CHUNK)
                frames.append(data)
            
            # ä¿å­˜ä¸ºWAVæ–‡ä»¶
            if frames:  # ç¡®ä¿æœ‰æ•°æ®å†ä¿å­˜
                wf = wave.open(filename, 'wb')
                wf.setnchannels(CHANNELS)
                wf.setsampwidth(audio.get_sample_size(FORMAT))
                wf.setframerate(RATE)
                wf.writeframes(b''.join(frames))
                wf.close()
                
                # å°†æ–‡ä»¶åæ·»åŠ åˆ°å¤„ç†é˜Ÿåˆ—
                audio_queue.put(filename)
                print(f"æ–‡ä»¶å·²ä¿å­˜: {filename}")
    
    finally:
        # åœæ­¢å’Œå…³é—­æµ
        stream.stop_stream()
        stream.close()
        audio.terminate()
        print("å½•éŸ³çº¿ç¨‹å·²ç»“æŸ")

def remove_tail_repeats_auto(s: str) -> str:
    """
    è‡ªåŠ¨åˆ¤æ–­é‡å¤ç‰‡æ®µé•¿åº¦ï¼Œä»åå‘å‰åˆ é™¤è¿ç»­é‡å¤çš„å­—ç¬¦ä¸²æ®µï¼Œåªä¿ç•™ç¬¬ä¸€æ¬¡å‡ºç°ã€‚
    æ”¯æŒå¤šç§é•¿åº¦çš„é‡å¤é€’å½’å»é‡ã€‚
    """
    n = len(s)
    changed = True
    while changed:
        changed = False
        for seg_len in range(n // 2, 0, -1):
            # æ£€æŸ¥ç»“å°¾æ˜¯å¦æœ‰è¿ç»­é‡å¤
            last_seg = s[-seg_len:]
            count = 1
            for i in range(2, n // seg_len + 1):
                start = -i * seg_len
                end = -(i - 1) * seg_len if -(i - 1) * seg_len != 0 else None
                if s[start:end] == last_seg:
                    count += 1
                else:
                    break
            if count > 1:
                cutoff = len(s) - (count - 1) * seg_len
                s = s[:cutoff]
                n = len(s)
                changed = True
                break  # é‡æ–°ä»æœ€å¤§é•¿åº¦å¼€å§‹
    return s

def join_segments_text(segments):
    """
    æ¥æ”¶ä¸€ä¸ªåŒ…å«è‹¥å¹²å­—å…¸ï¼ˆæ¯ä¸ªå­—å…¸æ˜¯ä¸€ä¸ªsegmentï¼Œå«'text'å­—æ®µï¼‰çš„åˆ—è¡¨ï¼Œ
    è¿”å›æ‰€æœ‰textå­—æ®µæ‹¼æ¥çš„å­—ç¬¦ä¸²ï¼Œç”¨é€—å·åˆ†éš”ã€‚
    """
    texts = [segment['text'] for segment in segments]
    return 'ï¼Œ'.join(texts)  # ä½¿ç”¨ä¸­æ–‡é€—å·æ‹¼æ¥

def clean_mermaid_to_lr(text: str) -> str:
    """
    æ¸…ç† Mermaid å›¾å†…å®¹ï¼š
    - åˆ é™¤ ```mermaid å’Œ ```
    - å°† graph TD æ›¿æ¢ä¸º graph LR
    """
    lines = text.strip().splitlines()
    cleaned_lines = []
    inside_code_block = False

    for line in lines:
        stripped = line.strip()
        if stripped == "```mermaid":
            inside_code_block = True
            continue
        elif stripped == "```":
            inside_code_block = False
            continue
        elif inside_code_block:
            if stripped.startswith("graph TD"):
                cleaned_lines.append(stripped.replace("graph TD", "graph LR", 1))
            else:
                cleaned_lines.append(line)
    
    return "\n".join(cleaned_lines)

def process_audio(whisperModel):
    """å¤„ç†çº¿ç¨‹ï¼šå¤„ç†é˜Ÿåˆ—ä¸­çš„éŸ³é¢‘æ–‡ä»¶å¹¶åˆ é™¤"""
    print("å¤„ç†çº¿ç¨‹å·²å¯åŠ¨ï¼Œç­‰å¾…æ–‡ä»¶...")
    text = ""
    last_qwen_time = 0
    while True:
        if exitFlag:
            print("process_audioå¤„ç†çº¿ç¨‹é€€å‡º")
            break
        if not globalConfig.running:
            text = ""   
            # æ¸…ç©ºæ‰€æœ‰å¾…å¤„ç†å†…å®¹
            while not audio_queue.empty():
                try:
                    filename = audio_queue.get_nowait()
                    audio_queue.task_done()
                    os.remove(filename)
                    print(f"æ¸…ç†: {filename}")
                except queue.Empty:
                    break
            time.sleep(1)
            continue
        try:
            # ä»é˜Ÿåˆ—ä¸­è·å–æ–‡ä»¶å
            filename = audio_queue.get()
            
            # è¿™é‡Œæ·»åŠ ä½ çš„éŸ³é¢‘å¤„ç†ä»£ç 
            print(f"æ­£åœ¨å¤„ç†æ–‡ä»¶: {filename}")

            resWhisper = whisperModel.transcribe_audio(filename)
            resWhisperText = remove_tail_repeats_auto(join_segments_text(resWhisper["segments"]))
            if(resWhisperText == ''):
                print("å†…å®¹ä¸ºç©º")
                # æ ‡è®°ä»»åŠ¡å®Œæˆå¹¶åˆ é™¤æ–‡ä»¶
                audio_queue.task_done()
                os.remove(filename)
                print(f"æ–‡ä»¶å·²åˆ é™¤: {filename}")
                continue
            text += resWhisperText
            #textå†™å…¥æ–‡æœ¬
            with open('out.txt', "w") as f:
                f.write(text)
            print(text)
            # Qwenç”Ÿæˆé—´éš”æ§åˆ¶
            now = time.time()
            if now - last_qwen_time >= globalConfig.qwen_interval:
                print(f"æ–‡æœ¬é•¿åº¦: {len(text)}")
                
                # ä½¿ç”¨ç®€åŒ–çš„ä»»åŠ¡å‡½æ•°
                try:
                    res = generate_mermaid(text, len(text)*3)
                    last_qwen_time = now
                    
                    if res:
                        # æŠŠresçš„å†…å®¹å†™å…¥æ–‡ä»¶
                        os.makedirs('/data/mermaidRender/text', exist_ok=True)
                        with open('/data/mermaidRender/dist/text/outmermaid.mmd', "w") as f:
                            f.write(clean_mermaid_to_lr(res))
                        print("æ€ç»´å¯¼å›¾å·²æ›´æ–°")
                    else:
                        print("æ€ç»´å¯¼å›¾ç”Ÿæˆå¤±è´¥")
                        
                except Exception as e:
                    print(f"Qwen å¤„ç†å¤±è´¥: {e}")
                
            print(f"æ–‡ä»¶å¤„ç†å®Œæˆ: {filename}")
            print(f"å¤„ç†ç»“æœ: {text}")

            # æ ‡è®°ä»»åŠ¡å®Œæˆ
            audio_queue.task_done()
            os.remove(filename)
            print(f"æ–‡ä»¶å·²åˆ é™¤: {filename}")
        except queue.Empty:
            # é˜Ÿåˆ—ä¸ºç©ºï¼Œç­‰å¾…globalConfig.runningä¸ºTrueå†ç»§ç»­
            time.sleep(1)
            continue
        except Exception as e:
            print(f"å¤„ç†éŸ³é¢‘æ—¶å‡ºé”™: {e}")
            # ç¡®ä¿ä»»åŠ¡è¢«æ ‡è®°ä¸ºå®Œæˆï¼Œå³ä½¿å‡ºé”™
            try:
                audio_queue.task_done()
                if 'filename' in locals() and os.path.exists(filename):
                    os.remove(filename)
            except:
                pass

def initModel():
    """åˆå§‹åŒ–æ¨¡å‹"""
    print("=== åˆå§‹åŒ–æ¨¡å‹ ===")
    
    # é¦–å…ˆæ£€æŸ¥å…±äº«ç®¡ç†å™¨çŠ¶æ€
    status = simple_qwen.get_status()
    print(f"å…±äº«ç®¡ç†å™¨åˆå§‹çŠ¶æ€: {status}")
    
    # åˆå§‹åŒ– Qwen æ¨¡å‹ï¼ˆå¦‚æœå°šæœªåŠ è½½ï¼‰
    if not simple_qwen.is_loaded():
        print("ğŸ”„ åˆå§‹åŒ– Qwen æ¨¡å‹...")
        initMermaid("åŠ è½½Qwenæ¨¡å‹...")
        
        class Args:
            model_path = "/data/qwen4btune_w4bf16_seq8192_bm1684x_1dev_20250721_195513.bmodel"
            config_path = "/data/LLM-TPU/models/Qwen3/python_demo/config"
            devid = "0"
            temperature = 0.5
            top_p = 1.0
            repeat_penalty = 1.8
            repeat_last_n = 32
            max_new_tokens = 1024
            generation_mode = "greedy"
            prompt_mode = "prompted"
            enable_history = False

        args = Args()
        
        try:
            simple_qwen.initialize_model(args)
            print("âœ… Qwen æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
        except Exception as e:
            print(f"âŒ Qwen æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    else:
        print("âœ… Qwen æ¨¡å‹å·²ç»åŠ è½½")
    
    # åˆå§‹åŒ– Whisper æ¨¡å‹
    print("ğŸ”„ åˆå§‹åŒ– Whisper æ¨¡å‹...")
    initMermaid("åŠ è½½Whisperæ¨¡å‹...")
    
    try:
        from transcribef import TranscribeWorker
        worker = TranscribeWorker()
        print("âœ… Whisper æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
    except Exception as e:
        print(f"âŒ Whisper æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
        raise
    
    initMermaid("")
    
    # éªŒè¯æœ€ç»ˆçŠ¶æ€
    final_status = simple_qwen.get_status()
    print(f"æ¨¡å‹åˆå§‹åŒ–å®ŒæˆåçŠ¶æ€: {final_status}")
    
    return worker

if __name__ == "__main__":
    try:
        print("ğŸš€ å¯åŠ¨è¯­éŸ³è¯†åˆ«æ€ç»´å¯¼å›¾ç³»ç»Ÿ...")
        
        # åˆå§‹åŒ–ç¯å¢ƒ
        initEnv()
        
        # åˆå§‹åŒ–æ¨¡å‹
        worker = initModel()
        
        # å¯åŠ¨APIæœåŠ¡å™¨ï¼ˆåœ¨å•ç‹¬çº¿ç¨‹ä¸­ï¼‰
        api_thread = threading.Thread(target=start_api_server, daemon=True)
        api_thread.start()
        
        # åˆ›å»ºå¹¶å¯åŠ¨å½•éŸ³çº¿ç¨‹
        record_thread = threading.Thread(target=record_audio)
        record_thread.start()
        
        # åˆ›å»ºå¹¶å¯åŠ¨å¤„ç†çº¿ç¨‹ - æ³¨æ„è¿™é‡Œåªä¼ å…¥ worker
        process_thread = threading.Thread(target=process_audio, args=(worker,))
        process_thread.start()

        # åˆ›å»ºå¹¶å¯åŠ¨é…ç½®ç›‘æ§çº¿ç¨‹
        config_thread = threading.Thread(target=globalConfig.config_watcher, daemon=True)
        config_thread.start()
        
        print("ç³»ç»Ÿå¯åŠ¨å®Œæˆï¼æŒ‰ Ctrl+C é€€å‡º...")
        print("=" * 50)
        
        # ä¸»çº¿ç¨‹ç­‰å¾…ç”¨æˆ·ä¸­æ–­
        while True:
            time.sleep(1)
            
    except KeyboardInterrupt:
        print("\næ­£åœ¨åœæ­¢æ‰€æœ‰çº¿ç¨‹...")
        exitFlag = True
        
        # ç­‰å¾…çº¿ç¨‹ç»“æŸ
        print("ç­‰å¾…å½•éŸ³çº¿ç¨‹ç»“æŸ...")
        record_thread.join(timeout=3)
        
        print("ç­‰å¾…å¤„ç†çº¿ç¨‹ç»“æŸ...")
        process_thread.join(timeout=3)
        
        # æ¸…ç†å‰©ä½™æ–‡ä»¶
        print("æ¸…ç†å‰©ä½™æ–‡ä»¶...")
        while not audio_queue.empty():
            try:
                filename = audio_queue.get_nowait()
                audio_queue.task_done()
                if os.path.exists(filename):
                    os.remove(filename)
                print(f"æ¸…ç†: {filename}")
            except queue.Empty:
                break
            except Exception as e:
                print(f"æ¸…ç†æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        
        # å…³é—­å…±äº«ç®¡ç†å™¨
        simple_qwen.shutdown()
        
        print("æ‰€æœ‰çº¿ç¨‹å·²åœæ­¢ï¼Œç¨‹åºé€€å‡º")
        
    except Exception as e:
        print(f"ç¨‹åºè¿è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        
        # ç¡®ä¿æ¸…ç†
        exitFlag = True
        simple_qwen.shutdown()
'''
'''shared_qwen_manager.py
import os
import json
import threading
from typing import Optional
from pipelinef import Qwen2

class SimpleQwenManager:
    """ç®€åŒ–çš„ Qwen æ¨¡å‹ç®¡ç†å™¨ - ä»…ç”¨äºæœ¬åœ°ç®¡ç†"""
    
    _instance: Optional['SimpleQwenManager'] = None
    _lock = threading.Lock()
    
    # çŠ¶æ€æ–‡ä»¶
    STATUS_FILE = "/tmp/qwen_model_status.json"
    
    def __new__(cls):
        with cls._lock:
            if cls._instance is None:
                cls._instance = super().__new__(cls)
                cls._instance._initialized = False
            return cls._instance
    
    def __init__(self):
        if self._initialized:
            return
            
        self.model: Optional[Qwen2] = None
        self.model_lock = threading.RLock()
        self._initialized = True
    
    def _write_status_file(self, status):
        """å†™å…¥çŠ¶æ€æ–‡ä»¶"""
        try:
            with open(self.STATUS_FILE, 'w') as f:
                json.dump(status, f, indent=2)
        except Exception as e:
            print(f"å†™å…¥çŠ¶æ€æ–‡ä»¶å¤±è´¥: {e}")
    
    def _read_status_file(self):
        """è¯»å–çŠ¶æ€æ–‡ä»¶"""
        try:
            if os.path.exists(self.STATUS_FILE):
                with open(self.STATUS_FILE, 'r') as f:
                    return json.load(f)
        except:
            pass
        return {"model_loaded": False, "pid": None, "model_path": None}
    
    def initialize_model(self, args):
        """åˆå§‹åŒ–æ¨¡å‹"""
        with self.model_lock:
            if self.model is None:
                print("ğŸ”„ åˆå§‹åŒ– Qwen æ¨¡å‹...")
                self.model = Qwen2(args)
                
                # æ›´æ–°çŠ¶æ€æ–‡ä»¶
                status = {
                    "model_loaded": True,
                    "pid": os.getpid(),
                    "model_path": args.model_path,
                    "timestamp": __import__('time').time()
                }
                self._write_status_file(status)
                
                print("âœ… Qwen æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
            else:
                print("âœ… æ¨¡å‹å·²ç»åŠ è½½")
    
    def get_model(self):
        """è·å–æ¨¡å‹å®ä¾‹"""
        return self.model
    
    def is_loaded(self):
        """æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²åŠ è½½"""
        return self.model is not None
    
    def get_status(self):
        """è·å–çŠ¶æ€ï¼ˆä¸»è¦ç”¨äºå…¼å®¹æ€§ï¼‰"""
        status = self._read_status_file()
        return {
            "model_loaded": self.model is not None,
            "local_model": self.model is not None,
            "external_model": status.get("model_loaded", False) and status.get("pid") != os.getpid(),
            "external_pid": status.get("pid"),
            "current_pid": os.getpid()
        }
    
    def shutdown(self):
        """å…³é—­ç®¡ç†å™¨"""
        if self.model is not None:
            print("ğŸ”„ æ¸…ç†æ¨¡å‹çŠ¶æ€...")
            self._write_status_file({"model_loaded": False, "pid": None, "model_path": None})
        print("âœ… æ¨¡å‹ç®¡ç†å™¨å·²å…³é—­")

# å…¨å±€å•ä¾‹
simple_qwen = SimpleQwenManager()

# ç®€åŒ–çš„ä»»åŠ¡å‡½æ•°
def generate_mermaid(text, maxWord):
    """æ€ç»´å¯¼å›¾ç”Ÿæˆ"""
    if simple_qwen.model:
        return simple_qwen.model.chat(text, maxWord)
    else:
        raise RuntimeError("æ¨¡å‹æœªåŠ è½½")

def generate_text(prompt, max_new_tokens=512):
    if simple_qwen.model:
        # model.generate_text ç°åœ¨æ˜¯ä¸€ä¸ªç”Ÿæˆå™¨ï¼Œæˆ‘ä»¬ç”¨ yield from å°†å…¶å†…å®¹ç»§ç»­å‘å¤–ä¼ 
        yield from simple_qwen.model.generate_text(prompt, max_new_tokens)
    else:
        raise RuntimeError("æ¨¡å‹æœªåŠ è½½")
'''
'''qwen_llm_adapter.py
"""
Qwen LLM bmodel é€‚é…å™¨ - ä½¿ç”¨HTTP APIè®¿é—®è¿œç¨‹æ¨¡å‹
ä¸º LightRAG æä¾›æœ¬åœ° Qwen bmodel æ¨ç†æ¥å£
"""
import sys
import os
import asyncio
import signal
import requests
import json
import time
from typing import List, Optional, AsyncIterator

# å¯¼å…¥ç®€åŒ–çš„å…±äº«ç®¡ç†å™¨ç”¨äºçŠ¶æ€æ£€æŸ¥
sys.path.append('/data/whisper-TPU_py/bmwhisper')
from shared_qwen_manager import simple_qwen

# ============ æ¨¡å‹è·¯å¾„é…ç½® ============
MODEL_CONFIG = {
    "llm_model_path": "/data/qwen4btune_w4bf16_seq8192_bm1684x_1dev_20250721_195513.bmodel",
    "config_path": "/data/LLM-TPU/models/Qwen3/python_demo/config",
    "device_id": "0",
    "temperature": 0.5,
    "top_p": 1.0,
    "repeat_penalty": 1.0,
    "repeat_last_n": 32,
    "max_new_tokens": 512,
    "generation_mode": "greedy"
}
# =====================================

# APIé…ç½®
API_BASE_URL = "http://localhost:8899"

class QwenLLMAdapter:
    """Qwen LLM bmodel é€‚é…å™¨ç±» - é€šè¿‡HTTP APIè®¿é—®æ¨¡å‹"""
    
    def __init__(self, **kwargs):
        self.config = {
            "model_path": kwargs.get("model_path", MODEL_CONFIG["llm_model_path"]),
            "config_path": kwargs.get("config_path", MODEL_CONFIG["config_path"]),
            "device_id": kwargs.get("device_id", MODEL_CONFIG["device_id"]),
            "temperature": kwargs.get("temperature", MODEL_CONFIG["temperature"]),
            "top_p": kwargs.get("top_p", MODEL_CONFIG["top_p"]),
            "repeat_penalty": kwargs.get("repeat_penalty", MODEL_CONFIG["repeat_penalty"]),
            "repeat_last_n": kwargs.get("repeat_last_n", MODEL_CONFIG["repeat_last_n"]),
            "max_new_tokens": kwargs.get("max_new_tokens", MODEL_CONFIG["max_new_tokens"]),
            "generation_mode": kwargs.get("generation_mode", MODEL_CONFIG["generation_mode"]),
            "prompt_mode": "prompted",
            "enable_history": False
        }
        
        self._check_model_availability()
    
    def _check_api_server(self, timeout=10):
        """æ£€æŸ¥APIæœåŠ¡å™¨æ˜¯å¦å¯ç”¨"""
        try:
            response = requests.get(f"{API_BASE_URL}/status", timeout=timeout)
            if response.status_code == 200:
                status = response.json()
                return True, status
            else:
                return False, f"HTTP {response.status_code}"
        except requests.exceptions.Timeout:
            return False, "è¿æ¥è¶…æ—¶"
        except requests.exceptions.ConnectionError:
            return False, "è¿æ¥æ‹’ç»"
        except Exception as e:
            return False, str(e)
    
    def _check_model_availability(self):
        """æ£€æŸ¥æ¨¡å‹å¯ç”¨æ€§"""
        # é¦–å…ˆæ£€æŸ¥ç®€åŒ–çš„å…±äº«ç®¡ç†å™¨çŠ¶æ€
        status = simple_qwen.get_status()
        print(f"å…±äº«ç®¡ç†å™¨çŠ¶æ€: {status}")
        
        if status["external_model"]:
            print(f"âœ… æ£€æµ‹åˆ°å¤–éƒ¨è¿›ç¨‹(PID: {status['external_pid']})å·²åŠ è½½æ¨¡å‹")
            
            # æ£€æŸ¥APIæœåŠ¡å™¨æ˜¯å¦å¯ç”¨
            api_available, api_status = self._check_api_server(timeout=10)
            if api_available:
                print("âœ… APIæœåŠ¡å™¨å¯ç”¨ï¼Œå¯ä»¥è¿›è¡Œæ¨ç†")
            else:
                print(f"âš ï¸ APIæœåŠ¡å™¨ä¸å¯ç”¨: {api_status}")
                print("è¯·ç¡®ä¿ sample_audio.py æ­£åœ¨è¿è¡Œ")
       
        else:
            print("âš ï¸ æœªæ£€æµ‹åˆ°å·²åŠ è½½çš„æ¨¡å‹")
            print("è¯·å…ˆè¿è¡Œ sample_audio.py åŠ è½½æ¨¡å‹")
    
    def _call_api(self, prompt: str, max_new_tokens: int = 512, retries=3) -> str:
        """è°ƒç”¨APIè¿›è¡Œæ¨ç†ï¼Œå¸¦é‡è¯•æœºåˆ¶"""
        
        for attempt in range(retries):
            try:
                print(f"ğŸ“¡ å‘é€APIè¯·æ±‚ (å°è¯• {attempt + 1}/{retries})")
                
                data = {
                    "prompt": prompt,
                    "max_new_tokens": max_new_tokens
                }
                
                # å¢åŠ è¶…æ—¶æ—¶é—´
                response = requests.post(
                    f"{API_BASE_URL}/generate",
                    json=data,
                    timeout=180  # 3åˆ†é’Ÿè¶…æ—¶
                )
                
                if response.status_code == 200:
                    result = response.json()
                    if result.get("status") == "success":
                        print("âœ… APIè¯·æ±‚æˆåŠŸ")
                        return result.get("result", "")
                    else:
                        error_msg = result.get('error', 'Unknown error')
                        print(f"âŒ APIè¿”å›é”™è¯¯: {error_msg}")
                        if attempt < retries - 1:
                            print(f"ğŸ”„ {5}ç§’åé‡è¯•...")
                            time.sleep(5)
                            continue
                        return f"API Error: {error_msg}"
                else:
                    error_msg = f"HTTP {response.status_code}"
                    print(f"âŒ HTTPé”™è¯¯: {error_msg}")
                    if attempt < retries - 1:
                        print(f"ğŸ”„ {3}ç§’åé‡è¯•...")
                        time.sleep(3)
                        continue
                    return f"HTTP Error: {error_msg}"
                    
            except requests.exceptions.ConnectionError:
                error_msg = "æ— æ³•è¿æ¥åˆ°APIæœåŠ¡å™¨"
                print(f"âŒ è¿æ¥é”™è¯¯: {error_msg}")
                if attempt < retries - 1:
                    print(f"ğŸ”„ {3}ç§’åé‡è¯•...")
                    time.sleep(3)
                    continue
                return f"Error: {error_msg}ã€‚è¯·ç¡®ä¿ sample_audio.py æ­£åœ¨è¿è¡Œã€‚"
            except requests.exceptions.Timeout:
                error_msg = "APIè¯·æ±‚è¶…æ—¶"
                print(f"âŒ è¶…æ—¶é”™è¯¯: {error_msg}")
                if attempt < retries - 1:
                    print(f"ğŸ”„ {5}ç§’åé‡è¯•...")
                    time.sleep(5)
                    continue
                return f"Error: {error_msg}"
            except Exception as e:
                error_msg = str(e)
                print(f"âŒ å…¶ä»–é”™è¯¯: {error_msg}")
                if attempt < retries - 1:
                    print(f"ğŸ”„ {3}ç§’åé‡è¯•...")
                    time.sleep(3)
                    continue
                return f"Error: {error_msg}"
        
        return "Error: æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†"
    
    def generate(self, prompt: str, system_prompt: str = None, 
                history_messages: list = None, stream: bool = False, **kwargs) -> str:
        """ç”Ÿæˆå›å¤ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰"""
        try:
            # æ„å»ºå®Œæ•´çš„æç¤º
            full_prompt = prompt
            if system_prompt:
                full_prompt = f"System: {system_prompt}\nUser: {prompt}"
            
            
            status = simple_qwen.get_status()
            
            # ä½¿ç”¨APIè°ƒç”¨
            print("ğŸ”„ ä½¿ç”¨APIæ¨ç†...")
            return self._call_api(
                full_prompt,
                kwargs.get('max_new_tokens', self.config['max_new_tokens'])
            )
            
        except Exception as e:
            return f"Error: {str(e)}"
    
    async def agenerate(self, prompt: str, system_prompt: str = None, 
                   history_messages: list = None, stream: bool = False, **kwargs) -> str:
        if stream:
            # è¿™ä¸ªåˆ†æ”¯å®é™…ä¸Šä¸ä¼šè¢« lightrag_bmodel2.py è°ƒç”¨ï¼Œä½†ä¿æŒé€»è¾‘å®Œæ•´
            raise NotImplementedError("Use agenerate_stream directly for streaming.")

        response_parts = []
        async for chunk in self.agenerate_stream(prompt, system_prompt, history_messages, **kwargs):
            response_parts.append(chunk)
        
        return "".join(response_parts)
    
    # æ›¿æ¢åŸæœ‰çš„ agenerate_stream
    async def agenerate_stream(self, prompt: str, system_prompt: str = None, 
                            history_messages: list = None, **kwargs) -> AsyncIterator[str]:
        full_prompt = prompt
        if system_prompt:
            full_prompt = f"System: {system_prompt}\nUser: {prompt}"
        
        data = {
            "prompt": full_prompt,
            "max_new_tokens": kwargs.get('max_new_tokens', self.config['max_new_tokens'])
        }

        loop = asyncio.get_running_loop()

        try:
            # åœ¨çº¿ç¨‹æ± ä¸­è¿è¡ŒåŒæ­¥çš„ requests ä»£ç ï¼Œé¿å…é˜»å¡äº‹ä»¶å¾ªç¯
            def blocking_request():
                response = requests.post(
                    f"{API_BASE_URL}/generate",
                    json=data,
                    timeout=180,
                    stream=True  # å…³é”®ï¼šå¼€å¯æµå¼è¯·æ±‚
                )
                response.raise_for_status() # å¦‚æœHTTPçŠ¶æ€ç æ˜¯4xx/5xxï¼Œåˆ™æŠ›å‡ºå¼‚å¸¸
                return response.iter_content(chunk_size=None, decode_unicode=True)

            # iter_content æ˜¯ä¸€ä¸ªåŒæ­¥è¿­ä»£å™¨ï¼Œæˆ‘ä»¬éœ€è¦åœ¨å¼‚æ­¥å‡½æ•°ä¸­åŒ…è£…å®ƒ
            sync_iterator = await loop.run_in_executor(None, blocking_request)
            
            for chunk in sync_iterator:
                yield chunk
                await asyncio.sleep(0) # è®©å‡ºæ§åˆ¶æƒï¼Œä¿æŒå¼‚æ­¥ç¯å¢ƒçš„å“åº”æ€§
        
        except requests.exceptions.RequestException as e:
            error_msg = f"Error during API stream request: {e}"
            print(error_msg)
            yield error_msg
        except Exception as e:
            yield f"Error: {str(e)}"

# å…¨å±€é€‚é…å™¨å®ä¾‹
_global_llm_adapter: Optional[QwenLLMAdapter] = None

def get_qwen_llm_adapter(model_path: str = None,
                        config_path: str = None,
                        **kwargs) -> QwenLLMAdapter:
    """è·å–å…¨å±€ LLM é€‚é…å™¨å®ä¾‹"""
    global _global_llm_adapter
    
    if _global_llm_adapter is None:
        config_kwargs = kwargs.copy()
        if model_path:
            config_kwargs["model_path"] = model_path
        if config_path:
            config_kwargs["config_path"] = config_path
            
        _global_llm_adapter = QwenLLMAdapter(**config_kwargs)
    
    return _global_llm_adapter

async def qwen_llm_model_func(prompt: str,
                             system_prompt: str = None,
                             history_messages: List = None,
                             keyword_extraction: bool = False,
                             stream: bool = False,
                             hashing_kv=None,
                             model_path: str = None,
                             config_path: str = None,
                             **kwargs) -> str:
    """LightRAG å…¼å®¹çš„ LLM å‡½æ•°"""
    adapter = get_qwen_llm_adapter(
        model_path=model_path,
        config_path=config_path,
        **kwargs
    )
    
    if stream:
        async def stream_generator():
            async for chunk in adapter.agenerate_stream(
                prompt, system_prompt, history_messages, **kwargs
            ):
                yield chunk
        return stream_generator()
    else:
        return await adapter.agenerate(
            prompt, system_prompt, history_messages, stream, **kwargs
        )

async def test_streaming():
    print("ğŸ§ª æµ‹è¯•å¼‚æ­¥æµå¼ç”Ÿæˆ...")
    adapter = get_qwen_llm_adapter()
    prompt = "ç®€å•ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½"
    print(f"Prompt: {prompt}")
    
    response_stream = adapter.agenerate_stream(prompt)
    async for chunk in response_stream:
        print(chunk, end="", flush=True)
    print("\nâœ… æµå¼æµ‹è¯•å®Œæˆï¼")


if __name__ == "__main__":
           
    print("="*60)
    asyncio.run(test_streaming())
'''
'''pipelinef.py
import argparse
import chat
import time
from transformers import AutoTokenizer
import globalConfig

class Qwen2():

    def __init__(self, args):
        # devid
        self.devices = [int(d) for d in args.devid.split(",")]

        # load tokenizer
        print("Load " + args.config_path + " ...")
        self.tokenizer = AutoTokenizer.from_pretrained(args.config_path, trust_remote_code=True)

        # warm up
        self.tokenizer.decode([0])

        # preprocess parameters, such as prompt & tokenizer
        self.system_prompt = "You are a helpful assistant."
        self.history = [{"role": "system", "content": self.system_prompt}]
        self.EOS = self.tokenizer.eos_token_id
        self.enable_history = args.enable_history

        self.model = chat.Qwen()
        self.init_params(args)
        self.load_model(args.model_path)

    def load_model(self, model_path):
        load_start = time.time()
        self.model.init(self.devices, model_path)
        load_end = time.time()
        print(f"\nLoad Time: {(load_end - load_start):.3f} s")

    def init_params(self, args):
        self.model.temperature = args.temperature
        self.model.top_p = args.top_p
        self.model.repeat_penalty = args.repeat_penalty
        self.model.repeat_last_n = args.repeat_last_n
        self.model.max_new_tokens = args.max_new_tokens
        self.model.generation_mode = args.generation_mode

    def clear(self):
        self.history = [{"role": "system", "content": self.system_prompt}]

    def update_history(self):
        if self.model.token_length >= self.model.SEQLEN:
            print("... (reach the maximal length)", flush=True, end="")
            self.history = [{"role": "system", "content": self.system_prompt}]
        else:
            self.history.append({"role": "assistant", "content": self.answer_cur})

    def encode_tokens(self, content):
        """æ™®é€šæ–‡æœ¬å¯¹è¯ç¼–ç  - ç”¨äº LightRAG"""
        # ä¸º LightRAG ä½¿ç”¨ç®€å•çš„å¯¹è¯æ¨¡æ¿
        history = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ï¼Œè¯·ç®€æ´æ˜äº†åœ°å›ç­”é—®é¢˜ã€‚"},
            {"role": "user", "content": content}
        ]
        
        text = self.tokenizer.apply_chat_template(
            history,
            tokenize=False,
            add_generation_prompt=True,
            enable_thinking=False  # æ˜ç¡®å…³é—­ think æ¨¡å¼
        )
        
        tokens = self.tokenizer(text).input_ids
        return tokens
    
    def encode_tokens_mermaid(self, content):
        """ä¸“é—¨ç”¨äºæ€ç»´å¯¼å›¾ç”Ÿæˆçš„ç¼–ç æ–¹æ³•"""
        template = f'''
<|im_start|>system\nä½ æ˜¯ä¸€ä¸ªMermaidä»£ç ç”Ÿæˆä¸“å®¶ï¼Œç”¨æˆ·å°†è¾“å…¥ä¸€æ®µå½•éŸ³çš„è¯­éŸ³è¯†åˆ«ç»“æœï¼Œè¯·ä½ è¯¦ç»†çš„æ€»ç»“å¹¶ç»˜åˆ¶ä¸€å¼ æ€ç»´å¯¼å›¾ï¼Œç”¨Mermaidå½¢å¼ï¼Œç”Ÿæˆç¬¦åˆè¯­æ³•çš„Mermaidä»£ç ã€‚è¦æ±‚ï¼š\n1. ä»…è¾“å‡ºæ ‡å‡†Mermaidä»£ç ï¼Œä¸åŒ…å«ä»»ä½•è§£é‡Šæˆ–æ³¨é‡Šï¼›\n2. èŠ‚ç‚¹åç§°éœ€ç”¨ä¸­æ–‡ï¼Œä½†è¯­æ³•å…³é”®å­—ï¼ˆå¦‚graph TDã€-->ï¼‰ä¿æŒè‹±æ–‡ï¼›\n3. ä¸¥æ ¼éµå¾ªMermaidè¯­æ³•è§„èŒƒã€‚\n\næ³¨æ„è¯­æ³•ç»†èŠ‚ï¼š\n- èŠ‚ç‚¹å®šä¹‰æ ¼å¼ï¼šèŠ‚ç‚¹å[ä¸­æ–‡æ ‡ç­¾]ï¼Œå¦‚ A[ç”¨æˆ·ç™»å½•]\n- è¿æ¥ç¬¦ï¼šä½¿ç”¨ -->ã€---ã€==>|æ–‡å­—|== ç­‰æ ‡å‡†ç¬¦å·\n- å­å›¾éœ€ç”¨subgraphåŒ…è£¹å¹¶æ­£ç¡®é—­åˆ\n- æ–¹æ‹¬å·ä¸­çš„æ–‡å­—ï¼ˆå³èŠ‚ç‚¹æ ‡ç­¾ï¼‰å†…å®¹è¯·ç”¨å¼•å·åŒ…è£¹\n- èŠ‚ç‚¹åç§°è¯·ä½¿ç”¨ä¸ä¼šä¸mermaidè¯­æ³•å‘ç”Ÿå†²çªçš„æ— æ„ä¹‰å†…å®¹ï¼ŒèŠ‚ç‚¹éœ€è¦æ˜¾ç¤ºçš„å†…å®¹è¯·æ”¾åœ¨èŠ‚ç‚¹æ ‡ç­¾ï¼ˆå³åç§°åçš„ä¸­æ‹¬å·ï¼‰ä¸­\n\n*è¯·ç¡®ä¿ä½ ç”Ÿæˆçš„æ˜¯Mermaidä»£ç !ä½ åªéœ€è¦ç”ŸæˆMermaidä»£ç ï¼Œè¯·ä¸è¦é™„åŠ å…¶ä»–ä¿¡æ¯!Mermaidä»£ç è¯·æ”¾åˆ°ä»£ç å—ä¸­!*\n\nè¯­éŸ³è¯†åˆ«ç»“æœå¦‚ä¸‹ï¼š\n<|im_end|>\n<|im_start|>user\n
{content}
\n<|im_end|>\n<|im_start|>system\nç°åœ¨è¯·ä½ å°†ä¸Šè¿°å†…å®¹è¯¦ç»†çš„æ€»ç»“å¹¶ç»˜åˆ¶ä¸€å¼ æ€ç»´å¯¼å›¾ï¼Œç”¨Mermaidå½¢å¼ï¼Œç”Ÿæˆç¬¦åˆè¯­æ³•çš„Mermaidä»£ç ã€‚è¦æ±‚ï¼š\n1. ä»…è¾“å‡ºæ ‡å‡†Mermaidä»£ç ï¼Œä¸åŒ…å«ä»»ä½•è§£é‡Šæˆ–æ³¨é‡Šï¼›\n2. èŠ‚ç‚¹åç§°éœ€ç”¨ä¸­æ–‡ï¼Œä½†è¯­æ³•å…³é”®å­—ï¼ˆå¦‚graph TDã€-->ï¼‰ä¿æŒè‹±æ–‡ï¼›\n3. ä¸¥æ ¼éµå¾ªMermaidè¯­æ³•è§„èŒƒã€‚\n\næ³¨æ„è¯­æ³•ç»†èŠ‚ï¼š\n- èŠ‚ç‚¹å®šä¹‰æ ¼å¼ï¼šèŠ‚ç‚¹å[\"ä¸­æ–‡æ ‡ç­¾\"]ï¼Œå¦‚ A[\"ç”¨æˆ·ç™»å½•\"]\n- è¿æ¥ç¬¦ï¼šä½¿ç”¨ -->ã€---ã€==>|æ–‡å­—|== ç­‰æ ‡å‡†ç¬¦å·\n- å­å›¾éœ€ç”¨subgraphåŒ…è£¹å¹¶æ­£ç¡®é—­åˆ\n- æ–¹æ‹¬å·ä¸­çš„æ–‡å­—ï¼ˆå³èŠ‚ç‚¹æ ‡ç­¾ï¼‰è¯·ç”¨å¼•å·åŒ…è£¹\n- èŠ‚ç‚¹åç§°è¯·ä½¿ç”¨ä¸ä¼šä¸mermaidè¯­æ³•å‘ç”Ÿå†²çªçš„æ— æ„ä¹‰å†…å®¹ï¼ŒèŠ‚ç‚¹éœ€è¦æ˜¾ç¤ºçš„å†…å®¹è¯·æ”¾åœ¨èŠ‚ç‚¹æ ‡ç­¾ï¼ˆå³åç§°åçš„ä¸­æ‹¬å·ï¼‰ä¸­\n\n*è¯·ç¡®ä¿ä½ ç”Ÿæˆçš„æ˜¯Mermaidä»£ç !ä½ åªéœ€è¦ç”ŸæˆMermaidä»£ç ï¼Œè¯·ä¸è¦é™„åŠ å…¶ä»–ä¿¡æ¯!Mermaidä»£ç è¯·æ”¾åˆ°ä»£ç å—ä¸­!èŠ‚ç‚¹æ ‡ç­¾ä¸­çš„æ–‡å­—è¯·ä½¿ç”¨è‹±æ–‡å¼•å·\"\"åŒ…è£¹!*<|im_end|>\n<|im_start|>assistant\n<think>\n\n</think>\n\n
'''
        tokens = self.tokenizer(template.format(content = content),
                                padding=False,  # "longest",  # "max_length",
                                truncation=True,
                                max_length=5000,
                                return_special_tokens_mask=True).input_ids
        return tokens

    def chat(self, content, maxWord = 100000):
        """ä¸“é—¨ç”¨äºæ€ç»´å¯¼å›¾ç”Ÿæˆçš„èŠå¤©æ–¹æ³•"""
        tokens = self.encode_tokens_mermaid(content)
        
        # check tokens
        if not tokens:
            print("Sorry: your question is empty!!")
            return
        if len(tokens) > self.model.SEQLEN:
            print(
                "The maximum question length should be shorter than {} but we get {} instead."
                .format(self.model.SEQLEN, len(tokens)))
            return
        print("\nAnswer: ", end="")
        return self.stream_answer(tokens, maxWord)
    
    def generate_text(self, content, maxWord = 1024):
        """æ™®é€šæ–‡æœ¬ç”Ÿæˆæ–¹æ³• - ç”¨äº LightRAG"""
        tokens = self.encode_tokens(content)
        
        # check tokens
        if not tokens:
            print("Sorry: your question is empty!!")
            return "Error: Empty input"
        if len(tokens) > self.model.SEQLEN:
            print(
                "The maximum question length should be shorter than {} but we get {} instead."
                .format(self.model.SEQLEN, len(tokens)))
            return f"Error: Input too long ({len(tokens)} tokens > {self.model.SEQLEN})"
        
        # return self.stream_answer_RAG(tokens, maxWord)
        yield from self.stream_answer_RAG(tokens, maxWord)

    def stream_answer(self, tokens , maxWord = 100000):
        """
        Stream the answer for the given tokens.
        """
        tok_num = 0
        self.answer_cur = ""
        self.answer_token = []

        # First token
        first_start = time.time()
        token = self.model.forward_first(tokens)
        first_end = time.time()
        # Following tokens
        full_word_tokens = []
        last_state = True
        last_state_token = 0
        while token != self.EOS and self.model.token_length < self.model.SEQLEN:
            full_word_tokens.append(token)
            word = self.tokenizer.decode(full_word_tokens, skip_special_tokens=True)
            if " " in word:
                token = self.model.forward_next()
                tok_num += 1
                continue
            self.answer_token += full_word_tokens
            print(word, flush=True, end="")
            # print(repr(word), flush=True, end="")
            tok_num += 1
            full_word_tokens = []
            token = self.model.forward_next()

            if(last_state and not globalConfig.running):
                last_state_token = tok_num
                last_state = False
            # is_newline = (word == '\n')
            # print(f"\n {is_newline}yes{tok_num} tokens {last_state_token} last_state_tok {last_state}||{globalConfig.running}")
            if((tok_num > maxWord or not globalConfig.running)and "\n" in word):
                print("...find \n (reach the maximal length)", flush=True, end="")#å¯èƒ½å­˜åœ¨bugä¸è¿‡è€ƒè™‘åˆ°\nä¸€èˆ¬å‡ºç°åœ¨æœ€å
                break
            if(tok_num -last_state_token > 50 and not globalConfig.running):
                print("... (reach the maximal length break)", flush=True, end="")
                break
            if(tok_num > maxWord+50):
                print("... (reach the maximal length)", flush=True, end="")
                break
            

        # counting time
        next_end = time.time()
        first_duration = first_end - first_start
        next_duration = next_end - first_end
        tps = tok_num / next_duration


        if self.enable_history:
            self.answer_cur = self.tokenizer.decode(self.answer_token)
            self.update_history()
        else:
            self.clear()
        
        print()
        print(f"FTL: {first_duration:.3f} s")
        print(f"TPS: {tps:.3f} token/s")
        return self.tokenizer.decode(self.answer_token)

    def stream_answer_RAG(self, tokens, maxWord=100000):
        """
        Stream the answer for the given tokens. (ä¿®æ­£ç‰ˆ)
        """
        # åˆå§‹åŒ–éƒ¨åˆ†ä¿æŒä¸å˜
        tok_num = 0
        first_start = time.time()
        token = self.model.forward_first(tokens)
        first_end = time.time()

        # Following tokens
        full_word_tokens = []
        
        while token != self.EOS and self.model.token_length < self.model.SEQLEN:
            # 1. ç´¯åŠ å½“å‰çš„ token
            full_word_tokens.append(token)
            # 2. å°è¯•è§£ç 
            word = self.tokenizer.decode(full_word_tokens, skip_special_tokens=True)

            # 3. æ£€æŸ¥è§£ç æ˜¯å¦å®Œæ•´ã€‚å¦‚æœåŒ…å«æ›¿æ¢å­—ç¬¦' 'ï¼Œè¯´æ˜æ˜¯éƒ¨åˆ†è§£ç ï¼Œ
            #    éœ€è¦è·å–æ›´å¤štokenæ‰èƒ½ç»„æˆä¸€ä¸ªå®Œæ•´çš„å­—ç¬¦ã€‚
            if " " in word:
                # å¦‚æœä¸å®Œæ•´ï¼Œåˆ™è·å–ä¸‹ä¸€ä¸ªtokenï¼Œç„¶åç»§ç»­ä¸‹ä¸€æ¬¡å¾ªç¯ä»¥ç´¯åŠ 
                token = self.model.forward_next()
                tok_num += 1
                continue

            # 4. å¦‚æœä»£ç èƒ½æ‰§è¡Œåˆ°è¿™é‡Œï¼Œè¯´æ˜'word'æ˜¯ä¸€ä¸ªå¯è¯»çš„ã€å®Œæ•´çš„ç‰‡æ®µã€‚
            #    æˆ‘ä»¬å°†å…¶äº§å‡ºã€‚
            yield word
            
            # 5. é‡ç½® token ç¼“å†²åŒºï¼Œä¸ºä¸‹ä¸€ä¸ªè¯åšå‡†å¤‡
            full_word_tokens = []

            # 6. ã€å…³é”®ã€‘ä¸ºä¸‹ä¸€æ¬¡å¾ªç¯è·å–æ–°çš„ token
            token = self.model.forward_next()
            tok_num += 1

        # å¾ªç¯ç»“æŸåï¼Œå¯ä»¥æ‰“å°ä¸€äº›ç»Ÿè®¡ä¿¡æ¯ï¼Œä½†è¿™å¯¹äºæµå¼å‡½æ•°ä¸æ˜¯å¿…éœ€çš„
        next_end = time.time()
        first_duration = first_end - first_start
        next_duration = next_end - first_end
        if next_duration > 0:
            tps = tok_num / next_duration
            print(f"\nFTL: {first_duration:.3f} s, TPS: {tps:.3f} token/s")

        # å¯¹äºç”Ÿæˆå™¨ï¼Œä¸éœ€è¦è¿”å›ä»»ä½•å€¼ï¼Œä¹Ÿä¸éœ€è¦å¤„ç† history

def main():
    # å›ºå®šå‚æ•°ï¼Œæ— éœ€å‘½ä»¤è¡Œè§£æ
    class Args:
        model_path = "/data/qwen4btune_w4bf16_seq8192_bm1684x_1dev_20250721_195513.bmodel"
        config_path = "config"
        devid = "0"
        temperature = 1.0
        top_p = 1.0
        repeat_penalty = 1.2
        repeat_last_n = 64
        max_new_tokens = 1024
        generation_mode = "greedy"
        prompt_mode = "prompted"
        enable_history = False  # æˆ– Falseï¼Œæ ¹æ®éœ€è¦

    args = Args()
    model = Qwen2(args)
    print("--- å¼€å§‹æµ‹è¯•æµå¼è¾“å‡º ---")
    # è°ƒç”¨ç°åœ¨æ˜¯ç”Ÿæˆå™¨çš„ generate_text
    text_generator = model.generate_text("ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚")
    for word in text_generator:
        print(word, end="", flush=True)
    print("\n--- æµ‹è¯•ç»“æŸ ---")


if __name__ == "__main__":
    main()
'''
'''globalConfig.py
import time
import json
# æ§åˆ¶çº¿ç¨‹è¿è¡Œçš„æ ‡å¿—
running = True
qwen_interval = 30  # Qwenç”Ÿæˆæ—¶é—´é—´éš”ï¼Œç§’

CONFIG_PATH = "/data/mermaidRender/dist/config.json"

def load_config():
    """ä»é…ç½®æ–‡ä»¶è¯»å–é…ç½®ï¼Œè¿”å›å­—å…¸"""
    try:
        with open(CONFIG_PATH, "r") as f:
            config = json.load(f)
        return config
    except Exception as e:
        print(f"è¯»å–é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
        return {}

def config_watcher():
    """é…ç½®æ–‡ä»¶ç›‘æ§çº¿ç¨‹ï¼Œå®æ—¶æ›´æ–°å…¨å±€é…ç½®å˜é‡"""
    global running, qwen_interval
    last_config = {}
    print("é…ç½®ç›‘æ§çº¿ç¨‹å·²å¯åŠ¨...")
    while True:
        config = load_config()
        if config != last_config:
            if "running" in config:
                running = bool(config["running"])
                print(f"é…ç½®æ›´æ–°: running = {running}")
            if "qwen_interval" in config:
                qwen_interval = float(config["qwen_interval"])
                print(f"é…ç½®æ›´æ–°: qwen_interval = {qwen_interval}")
            last_config = config
        time.sleep(1)  # æ¯ç§’æ£€æŸ¥ä¸€æ¬¡
'''